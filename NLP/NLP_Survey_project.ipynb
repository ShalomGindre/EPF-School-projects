{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wQdJFK5Vy5hB",
        "bDiVi63d2N6Y",
        "c_D60dslzfhQ",
        "3tw0aQ5447ym",
        "o7dNlvi19SO3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQdJFK5Vy5hB"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MP9FeCwy8s1"
      },
      "source": [
        "Dans cet essai je vais essayer d'appliquer des fonctions présentées dans les cours de NLP. A la fin de ce notebook j'appliquerai les fonctions les plus intéressantes à notre dataset et j'essaierai d'améliorer notre code. \n",
        "\n",
        "Dans ce jupyter notebook, nous allons importer les data puis les séparer en trois groupes. Nous avons les formulaires des personnes concernées, de leur personnels soignants ainsi que de leur entourage. Nous allons analyzer chaque catégorie et comparer les résultats.  \n",
        "\n",
        "La première étape d'analyze est le nettoyage des données. Cette étape est longue et elle est une priorité dans notre projet. Nous voulons extraire des besoins de textes où il ne reste uniquement les informations les plus importantes. \n",
        "\n",
        "La seconde étape est d'examiner les mots restant à leur racine. Nous voulons épurer ces mots. Nous appelons ça la lemmatization. \n",
        "\n",
        "La prochaine étape se veut de nous aider à classifier les mots restants en les rassemblant en fonction de leur importance, leur sens et le sentiment que les phrases peuvent transmettre. Nous appelons ça le clustering. Nous allons mettre au point plusieurs différentes manières de trouver des résultats. Puis nous les comparerons. \n",
        "\n",
        "Les documents nécessaires pour faire tourner le jupyter notebook se trouvent dans le dossier Colab dans le projet git appelé \"Idlys\". Il vous suffit de le télécharger puis d'aller dans la banderole à gauche de votre écran sur Colab, de cliquer sur la quatrième catégorie appelée \"Dossiers\" ou \"Files\", puis d'importer vos fichiers. Il est important qu'ils ne soient pas importer via \"sample_data\" ou d'un drive car cela modifierait le chemin d'importation et les fichiers ne pourront être lu. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDiVi63d2N6Y"
      },
      "source": [
        "# Librairies & Importation des données "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21EMMNPcJexv"
      },
      "source": [
        "**Importation** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4IvsgtNI4EV"
      },
      "source": [
        "> Importation de base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjyAf01xI8eT"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import scipy\n",
        "import sklearn\n",
        "import pdb\n",
        "import pickle\n",
        "import string\n",
        "import time\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqKgEURII_Cv"
      },
      "source": [
        "> Importation nécessaire pour le NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ei7DfbC2QXr",
        "outputId": "b289da07-a173-4062-8359-024d0665add0"
      },
      "source": [
        "# lemmatizer spacy\n",
        "!pip install spacy-lefff\n",
        "import spacy\n",
        "from spacy_lefff import LefffLemmatizer, POSTagger\n",
        "\n",
        "\n",
        "# from nltk\n",
        "import nltk.corpus\n",
        "import nltk as nlp\n",
        "nltk.download('wordnet')\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "nltk.data.path.append('.')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# from gensim\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# from os \n",
        "from os import getcwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy-lefff in /usr/local/lib/python3.6/dist-packages (0.3.7)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-lefff) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (51.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.19.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-lefff) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-lefff) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-lefff) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-lefff) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-lefff) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-lefff) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-lefff) (3.4.0)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIze0e8LqknK",
        "outputId": "d3920bb1-2a20-49aa-b455-b4eab4b91fba"
      },
      "source": [
        "!python -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fr_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz#egg=fr_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQqL0-8dUWE8"
      },
      "source": [
        "import spacy\n",
        "\n",
        "fr_spaCy = spacy.load(\"fr\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01WLlqJKgx3U"
      },
      "source": [
        "#!pip install urllib3==1.25.10\n",
        "\n",
        "#!pip install smart_open==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0EgWRlZJMKA"
      },
      "source": [
        "> Importation des fonctions crées en annexe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGbnw8QKJc6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9309034f-2505-43f8-9504-d0cc75faa8c9"
      },
      "source": [
        "from nettoyage import stopwords\n",
        "from Frequence import count_words, get_words_with_nplus_frequency, count_n_grams, estimate_probability, estimate_probabilities\n",
        "from Pour_aller_plus_loin import make_count_matrix, make_probability_matrix\n",
        "from Neighboor import cosine_similarity, get_dict, get_document_embedding, get_document_vecs, hash_value_of_vector, make_hash_table, approximate_knn, nearest_neighbor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT90vhz-ei2E"
      },
      "source": [
        "**Concaténer les données et créer trois dataframes en fonction des intervenants**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZExSdNjGh7i0"
      },
      "source": [
        "Nous importons nos données. Nous avons donc trois sortes de formulaires. \n",
        "\n",
        "Nous avons : \n",
        "\n",
        "> df1 : réponses des personnes concernées \n",
        "\n",
        "> df2 : réponses de leur personnel soigant \n",
        "\n",
        "> df3 : réponses de leur entourage "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdkNPinS6PH6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "88530b17-5bb3-43f8-ec38-fd13a6603cff"
      },
      "source": [
        "# Readind the excel and turning it into a single string\n",
        "'''df1 = pd.read_excel('/content/sample_data/Copie-de-réponses-proches-51-100.xlsx', header = None)\n",
        "df1.head()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"df1 = pd.read_excel('/content/sample_data/Copie-de-réponses-proches-51-100.xlsx', header = None)\\ndf1.head()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8p8kY8YjvHn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1e63d225-2eed-4a93-d9e7-1864d33f2ee1"
      },
      "source": [
        "'''df2 = pd.read_excel('/content/sample_data/Copie-de-réponses-proches-51-100.xlsx', header = None)\n",
        "df2.head()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"df2 = pd.read_excel('/content/sample_data/Copie-de-réponses-proches-51-100.xlsx', header = None)\\ndf2.head()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njSXtCtHjxtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e45de4-785f-4c83-b56d-72b6b7a77fce"
      },
      "source": [
        "df3 = pd.read_excel('Copie-de-réponses-proches-51-100.xlsx', header = None)\n",
        "print(df3.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0  ...                                                  6\n",
            "0  N°  ...       Si seulement il existait un truc pour faire…\n",
            "1  51  ...    Un fauteuil électrique avec détection de danger\n",
            "2  52  ...  Militer pour que le langage des signes soit ad...\n",
            "3  53  ...  OUVRIR UNE PORTE D UN PLACAR AVEC UNE TELECOMM...\n",
            "4  58  ...  Il faut décomposer chaque geste construit en d...\n",
            "\n",
            "[5 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wNy-7067Cem"
      },
      "source": [
        "**Sélectionner les colonnes qui présentent des besoins** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XweXM2YcgTr3"
      },
      "source": [
        "Dans cette section nous sélectionnons les colonnes qui pourraient présenter des besoins. \n",
        "\n",
        "Nous avons trois catégories différentes de formulaires. Nous allons donc créer trois datasets différents pour chaque catégories contenant ces colonnes. \n",
        "\n",
        "> cat1 : catégorie 1, personne concernée \n",
        "\n",
        "> cat2 : catégorie 2, personnel soigant\n",
        "\n",
        "> cat3 : catégorie 3, entourage de personne concernée"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKDXSFD4bIjf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "afbdaef1-d66a-4df1-a8f4-efd347e2190b"
      },
      "source": [
        "'''cat1 = df1[6]\n",
        "cat1 =cat1.to_string(index=False)\n",
        "print(cat1)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cat1 = df1[6]\\ncat1 =cat1.to_string(index=False)\\nprint(cat1)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuuI5voA7AhM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fbe220eb-5716-42aa-d6a5-ae1455cbe2c1"
      },
      "source": [
        "'''cat2 = df2[6]\n",
        "cat2 =cat2.to_string(index=False)\n",
        "print(cat2)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cat2 = df2[6]\\ncat2 =cat2.to_string(index=False)\\nprint(cat2)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiwCwuy4hYBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6840103-6e0a-4089-9810-7aaeca783270"
      },
      "source": [
        "cat3 = df3[6]\n",
        "cat3 =cat3.to_string(index=False)\n",
        "print(cat3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Si seulement il existait un truc pour faire…\n",
            "   Un fauteuil électrique avec détection de danger\n",
            " Militer pour que le langage des signes soit ad...\n",
            " OUVRIR UNE PORTE D UN PLACAR AVEC UNE TELECOMM...\n",
            " Il faut décomposer chaque geste construit en d...\n",
            "                                               NaN\n",
            " Attacher le fauteuil dans le véhicule sans se ...\n",
            " pour transforme run fauteuil roulant en fauteu...\n",
            " Un déambulateur performant pliable pour aider ...\n",
            "                                               NaN\n",
            " Un truc pour alerter automatiquement le fourni...\n",
            " Un système pour permettre la lecture au lit (o...\n",
            " Un truc aussi pour pouvoir appeler au téléphon...\n",
            " S'il existait un truc pour aider une personne ...\n",
            " ne pas se contorsioner pour attacher le fauteu...\n",
            " Des parcs de jeux adaptés !!! DES plages avec ...\n",
            "                domotique pour fermer le domicile?\n",
            "                                               NaN\n",
            "                                                 X\n",
            " Pas de solutions pour attacher correctement mo...\n",
            "                                               NaN\n",
            "                           www.changing-places.org\n",
            "                                    Voir au dessus\n",
            "                                               NaN\n",
            "           Favoriser le hand sport dès tout petit.\n",
            " Si seulement lew parents pouvait etre plus aid...\n",
            " Oui enlever la Spasticité des membres inférieu...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o3eLXvLe8KC"
      },
      "source": [
        "# Nettoyage "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChEBPq9fIIew"
      },
      "source": [
        "Nous créons une fonction qui permet de nettoyer notre dataset. Cette fonction appelle d'autres fonctions crée dans d'autres fichiers.\n",
        "\n",
        "> Rappel : Ces fonctions se trouvent dans le fichier .py appelé \"Nettoyage\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23e8KV1wNeED"
      },
      "source": [
        "def clean(data): \n",
        "\n",
        "  # bags of words \n",
        "  data_1 = data.split()\n",
        "  #print(data_1)\n",
        "\n",
        "  # minuscule\n",
        "  data_2 = [word.lower() for word in data_1]\n",
        "  #print(data_2)\n",
        "\n",
        "  # enlever tous les stopwords \n",
        "  data_3 = stopwords(data_2)\n",
        "  #print(data_3)\n",
        "\n",
        "  # enlever tous les \"nan\" du texte\n",
        "  data_4 = [x for x in data_3 if str(x) != 'nan']\n",
        "  #print(data_4)\n",
        "\n",
        "  return data_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "316HUzhhfQ1r"
      },
      "source": [
        "Nous appliquons notre fonction à toutes les catégories. Nous créeons un nouveau data set pour chacune des catégories. \n",
        "\n",
        "> d1_net : données nettoyées catégorie 1 \n",
        "\n",
        "> d2_net : données nettoyées catégorie 2\n",
        "\n",
        "> d3_net : données nettoyées catégorie 3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PugpFl2uypPH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "315d59f8-27b6-418d-bbbc-1144014897a5"
      },
      "source": [
        "'''d1_net = clean(cat1)\n",
        "d2_net = clean(cat2)\n",
        "d3_net = clean(cat3)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'d1_net = clean(cat1)\\nd2_net = clean(cat2)\\nd3_net = clean(cat3)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1znu77g2_tvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e893dfa-a0d9-42d9-fc39-a4e04ac8b44b"
      },
      "source": [
        "d3_net = clean(cat3)\n",
        "print(d3_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['existait', 'truc', 'faire…', 'fauteuil', 'électrique', 'détection', 'danger', 'militer', 'langage', 'signes', 'ad...', 'ouvrir', 'porte', 'placar', 'telecomm...', 'faut', 'décomposer', 'geste', 'construit', 'd...', 'attacher', 'fauteuil', 'véhicule', '...', 'transforme', 'run', 'fauteuil', 'roulant', 'fauteu...', 'déambulateur', 'performant', 'pliable', 'aider', '...', 'truc', 'alerter', 'automatiquement', 'fourni...', 'système', 'permettre', 'lecture', 'lit', '(o...', 'truc', 'pouvoir', 'appeler', 'téléphon...', \"s'il\", 'existait', 'truc', 'aider', '...', 'contorsioner', 'attacher', 'fauteu...', 'parcs', 'jeux', 'adaptés', '!!!', 'plages', '...', 'domotique', 'fermer', 'domicile?', 'solutions', 'attacher', 'correctement', 'mo...', 'www.changing-places.org', 'voir', 'favoriser', 'hand', 'sport', 'petit.', 'lew', 'parents', 'pouvait', 'aid...', 'oui', 'enlever', 'spasticité', 'membres', 'inférieu...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRAxPqUEl4VG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9c-oXZ8pcoC"
      },
      "source": [
        "# Fréquence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcj5EQ99ABBG"
      },
      "source": [
        "Dans cette section nous voulons faire une comparaison sur les différents types de mots présents en fonction de chaque catégorie.\n",
        "\n",
        "Nous allons chercher à savoir la fréquence des mots dans chaque texte.Nous pourrions savoir si certains mots reviennent plus que d'autres dans chacune des catéogries ou par exemple si une catégorie utilisent plus certains mots en particulier.\n",
        "\n",
        "Nous allons procéder en plusieurs étapes. Nous voulons tout d'abord décompter chaque mot puis calculer leur fréquence. Dans un second temps, nous allons utiliser des foncitonnalités plus avancées comme la N-grams où nous pourrions décompter chaque mot mais aussi des couples de mots et pouvoir mieux analyzer notre texte. \n",
        "\n",
        "Puis nous allons voir comment nous pouvons afficher la fréquence des mots dans notre dataset. Nous allons d'abord réfléchir à un certain mot clé, une target et calculer sa fréquence. Puis nous allons générer cette opérations sur l'ensemble de notre dataset et calculer les probablités de chacun des mots. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T74T7kUIlu2e"
      },
      "source": [
        "def frequence(data):\n",
        "\n",
        "   # count chaque mot\n",
        "   data_1 = count_words(data)\n",
        "   print(\"Compte:\", data_1)\n",
        "\n",
        "   # fréquence de chaque mot \n",
        "   data_2 = get_words_with_nplus_frequency(data, count_threshold=2)\n",
        "   print(\"Fréquence:\", data_2)\n",
        "\n",
        "   # N-grams\n",
        "   data_3 = count_n_grams(data, 1)\n",
        "   print(\"N-grams:\", data_3)\n",
        "   data_32 = count_n_grams(data, 2)\n",
        "   print(\"N-grams par couples de mots:\", data_32)\n",
        "\n",
        "   # calculation probability target\n",
        "   #data_4 = estimate_probability(data)\n",
        "   unique_words = [d3_net[i] for i in range(0, 50)] # on prend les 50 premières lignes de notre dataset \n",
        "   data_4 = estimate_probability(\"faire\", \"truc\", data_3, data_32, len(unique_words), k=1)\n",
        "   print(\"Probabilité de la target:\", data_4)\n",
        "\n",
        "   # calcul probaility all words \n",
        "   unique_words = [d3_net[i] for i in range(0, 50)]\n",
        "   data_5 = estimate_probabilities(\"faire\", data_3, data_32, unique_words, k=1)\n",
        "   print(\"Probabilité de chacun des mots:\", data_5)\n",
        "\n",
        "   return \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFrciY4W_fGb"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8CEBAHE_eXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77dee0c-04b4-4a47-b05d-2f3b3ba043b0"
      },
      "source": [
        "Get_Freq = frequence(d3_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compte: {'existait': 2, 'truc': 4, 'faire…': 1, 'fauteuil': 3, 'électrique': 1, 'détection': 1, 'danger': 1, 'militer': 1, 'langage': 1, 'signes': 1, 'ad...': 1, 'ouvrir': 1, 'porte': 1, 'placar': 1, 'telecomm...': 1, 'faut': 1, 'décomposer': 1, 'geste': 1, 'construit': 1, 'd...': 1, 'attacher': 3, 'véhicule': 1, '...': 4, 'transforme': 1, 'run': 1, 'roulant': 1, 'fauteu...': 2, 'déambulateur': 1, 'performant': 1, 'pliable': 1, 'aider': 2, 'alerter': 1, 'automatiquement': 1, 'fourni...': 1, 'système': 1, 'permettre': 1, 'lecture': 1, 'lit': 1, '(o...': 1, 'pouvoir': 1, 'appeler': 1, 'téléphon...': 1, \"s'il\": 1, 'contorsioner': 1, 'parcs': 1, 'jeux': 1, 'adaptés': 1, '!!!': 1, 'plages': 1, 'domotique': 1, 'fermer': 1, 'domicile?': 1, 'solutions': 1, 'correctement': 1, 'mo...': 1, 'www.changing-places.org': 1, 'voir': 1, 'favoriser': 1, 'hand': 1, 'sport': 1, 'petit.': 1, 'lew': 1, 'parents': 1, 'pouvait': 1, 'aid...': 1, 'oui': 1, 'enlever': 1, 'spasticité': 1, 'membres': 1, 'inférieu...': 1}\n",
            "Fréquence: ['existait', 2, 'truc', 4, 'fauteuil', 3, 'attacher', 3, '...', 4, 'fauteu...', 2, 'aider', 2]\n",
            "N-grams: {('existait',): 2, ('truc',): 4, ('faire…',): 1, ('fauteuil',): 3, ('électrique',): 1, ('détection',): 1, ('danger',): 1, ('militer',): 1, ('langage',): 1, ('signes',): 1, ('ad...',): 1, ('ouvrir',): 1, ('porte',): 1, ('placar',): 1, ('telecomm...',): 1, ('faut',): 1, ('décomposer',): 1, ('geste',): 1, ('construit',): 1, ('d...',): 1, ('attacher',): 3, ('véhicule',): 1, ('...',): 4, ('transforme',): 1, ('run',): 1, ('roulant',): 1, ('fauteu...',): 2, ('déambulateur',): 1, ('performant',): 1, ('pliable',): 1, ('aider',): 2, ('alerter',): 1, ('automatiquement',): 1, ('fourni...',): 1, ('système',): 1, ('permettre',): 1, ('lecture',): 1, ('lit',): 1, ('(o...',): 1, ('pouvoir',): 1, ('appeler',): 1, ('téléphon...',): 1, (\"s'il\",): 1, ('contorsioner',): 1, ('parcs',): 1, ('jeux',): 1, ('adaptés',): 1, ('!!!',): 1, ('plages',): 1, ('domotique',): 1, ('fermer',): 1, ('domicile?',): 1, ('solutions',): 1, ('correctement',): 1, ('mo...',): 1, ('www.changing-places.org',): 1, ('voir',): 1, ('favoriser',): 1, ('hand',): 1, ('sport',): 1, ('petit.',): 1, ('lew',): 1, ('parents',): 1, ('pouvait',): 1, ('aid...',): 1, ('oui',): 1, ('enlever',): 1, ('spasticité',): 1, ('membres',): 1, ('inférieu...',): 1}\n",
            "N-grams par couples de mots: {('existait', 'truc'): 2, ('truc', 'faire…'): 1, ('faire…', 'fauteuil'): 1, ('fauteuil', 'électrique'): 1, ('électrique', 'détection'): 1, ('détection', 'danger'): 1, ('danger', 'militer'): 1, ('militer', 'langage'): 1, ('langage', 'signes'): 1, ('signes', 'ad...'): 1, ('ad...', 'ouvrir'): 1, ('ouvrir', 'porte'): 1, ('porte', 'placar'): 1, ('placar', 'telecomm...'): 1, ('telecomm...', 'faut'): 1, ('faut', 'décomposer'): 1, ('décomposer', 'geste'): 1, ('geste', 'construit'): 1, ('construit', 'd...'): 1, ('d...', 'attacher'): 1, ('attacher', 'fauteuil'): 1, ('fauteuil', 'véhicule'): 1, ('véhicule', '...'): 1, ('...', 'transforme'): 1, ('transforme', 'run'): 1, ('run', 'fauteuil'): 1, ('fauteuil', 'roulant'): 1, ('roulant', 'fauteu...'): 1, ('fauteu...', 'déambulateur'): 1, ('déambulateur', 'performant'): 1, ('performant', 'pliable'): 1, ('pliable', 'aider'): 1, ('aider', '...'): 2, ('...', 'truc'): 1, ('truc', 'alerter'): 1, ('alerter', 'automatiquement'): 1, ('automatiquement', 'fourni...'): 1, ('fourni...', 'système'): 1, ('système', 'permettre'): 1, ('permettre', 'lecture'): 1, ('lecture', 'lit'): 1, ('lit', '(o...'): 1, ('(o...', 'truc'): 1, ('truc', 'pouvoir'): 1, ('pouvoir', 'appeler'): 1, ('appeler', 'téléphon...'): 1, ('téléphon...', \"s'il\"): 1, (\"s'il\", 'existait'): 1, ('truc', 'aider'): 1, ('...', 'contorsioner'): 1, ('contorsioner', 'attacher'): 1, ('attacher', 'fauteu...'): 1, ('fauteu...', 'parcs'): 1, ('parcs', 'jeux'): 1, ('jeux', 'adaptés'): 1, ('adaptés', '!!!'): 1, ('!!!', 'plages'): 1, ('plages', '...'): 1, ('...', 'domotique'): 1, ('domotique', 'fermer'): 1, ('fermer', 'domicile?'): 1, ('domicile?', 'solutions'): 1, ('solutions', 'attacher'): 1, ('attacher', 'correctement'): 1, ('correctement', 'mo...'): 1, ('mo...', 'www.changing-places.org'): 1, ('www.changing-places.org', 'voir'): 1, ('voir', 'favoriser'): 1, ('favoriser', 'hand'): 1, ('hand', 'sport'): 1, ('sport', 'petit.'): 1, ('petit.', 'lew'): 1, ('lew', 'parents'): 1, ('parents', 'pouvait'): 1, ('pouvait', 'aid...'): 1, ('aid...', 'oui'): 1, ('oui', 'enlever'): 1, ('enlever', 'spasticité'): 1, ('spasticité', 'membres'): 1, ('membres', 'inférieu...'): 1, ('inférieu...',): 1}\n",
            "Probabilité de la target: 0.02\n",
            "Probabilité de chacun des mots: {'existait': 0.019230769230769232, 'truc': 0.019230769230769232, 'faire…': 0.019230769230769232, 'fauteuil': 0.019230769230769232, 'électrique': 0.019230769230769232, 'détection': 0.019230769230769232, 'danger': 0.019230769230769232, 'militer': 0.019230769230769232, 'langage': 0.019230769230769232, 'signes': 0.019230769230769232, 'ad...': 0.019230769230769232, 'ouvrir': 0.019230769230769232, 'porte': 0.019230769230769232, 'placar': 0.019230769230769232, 'telecomm...': 0.019230769230769232, 'faut': 0.019230769230769232, 'décomposer': 0.019230769230769232, 'geste': 0.019230769230769232, 'construit': 0.019230769230769232, 'd...': 0.019230769230769232, 'attacher': 0.019230769230769232, 'véhicule': 0.019230769230769232, '...': 0.019230769230769232, 'transforme': 0.019230769230769232, 'run': 0.019230769230769232, 'roulant': 0.019230769230769232, 'fauteu...': 0.019230769230769232, 'déambulateur': 0.019230769230769232, 'performant': 0.019230769230769232, 'pliable': 0.019230769230769232, 'aider': 0.019230769230769232, 'alerter': 0.019230769230769232, 'automatiquement': 0.019230769230769232, 'fourni...': 0.019230769230769232, 'système': 0.019230769230769232, 'permettre': 0.019230769230769232, 'lecture': 0.019230769230769232, 'lit': 0.019230769230769232, '(o...': 0.019230769230769232, 'pouvoir': 0.019230769230769232, 'appeler': 0.019230769230769232, 'téléphon...': 0.019230769230769232, \"s'il\": 0.019230769230769232, '<e>': 0.019230769230769232, '<oov>': 0.019230769230769232}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf55evefw_Uv"
      },
      "source": [
        "Dans cette section nous avons voulu explorer nos données. Nous chercher les mots qui apparaissaient le plus de fois puis leur fréquence exacte. Nous avons aussi introduit la fonctionnalité de N-grams qui permet de décompter tous les mots du dataset ainsi que de possibles couples de mots. Il est intéressant de voir que certains mots vont par pairs comme \"fauteuils roulants\" ou \"langue signes\". Ces couples peuvent avoir une fréquence élévée et peuvent nous réléver des indices sur les besoins ou les pricipaux thèmes des réponses. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO-aDaNEE9zr"
      },
      "source": [
        "# Pour aller plus loin "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krHmuVFOCuoY"
      },
      "source": [
        "> Pour aller plus loin et ainsi une meilleure compréhension, voici une courte documentation sur les fonctionnalités utilisées dans la fonction. \n",
        "\n",
        "> Rappel : toutes les fonctions utilisées dans cette section se trouvent dans les fichiers python (.py) appelés **Fréquence** et **Neighboor** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fku6laGQ9280"
      },
      "source": [
        "**N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_k52R7k-AMQ"
      },
      "source": [
        "\n",
        "\n",
        "Dans cette section, nous allons développer le modèle de langage n-grams.\n",
        "- Supposons que la probabilité du mot suivant ne dépende que du n-gramme précédent.\n",
        "- Le n-gramme précédent est la série des \"n\" mots précédents.\n",
        "\n",
        "La probabilité conditionnelle pour le mot à la position \"t\" dans la phrase, étant donné que les mots qui le précèdent sont $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ is:\n",
        "\n",
        "$$ P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}$$\n",
        "\n",
        "Nous pouvons estimer cette probabilité en comptant les occurrences de ces séries de mots dans les données de formation.\n",
        "- La probabilité peut être estimée sous la forme d'un rapport, où\n",
        "- Le numérateur est le nombre de fois que le mot \"t\" apparaît après les mots t-1 à t-n dans les données de formation.\n",
        "- Le dénominateur est le nombre de fois que les mots t-1 à t-n apparaissent dans les données d'entraînement.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
        "\n",
        "- La fonction $C(\\cdots)$ indique le nombre d'occurrences de la séquence donnée. \n",
        "- La fonction $\\hat{P}$ désigne l'estimation de $P$. \n",
        "- Notez que le dénominateur de l'équation (2) est le nombre d'occurrences des mots $n$ précédents, et le numérateur est la même séquence suivie du mot $w_t$.\n",
        "\n",
        "Plus tard, nous modifierons l'équation (2) en ajoutant un lissage k, qui évite les erreurs lorsque les comptes sont nuls.\n",
        "\n",
        "L'équation (2) nous dit que pour estimer les probabilités basées sur les n-grammes, nous avons besoin des nombres de n-grammes (pour le dénominateur) et de (n+1)-grammes (pour le numérateur).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jLltyhW8zMy"
      },
      "source": [
        "**Counts-grams**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIlMXUyM856-"
      },
      "source": [
        "Nous créons une fonction qui calcule les comptes de n-grammes pour un nombre arbitraire 𝑛. \n",
        "Lors du calcul du nombre de n-grammes, préparer la phrase à l'avance en préparant 𝑛-1\n",
        "\n",
        "des marqueurs de départ \"< s >\" pour indiquer le début de la phrase.\n",
        "\n",
        "    - Par exemple, dans le modèle du bi-gramme (N=2), une séquence avec deux marqueurs de départ \"<s><s>\" devrait prédire le premier mot d'une phrase.\n",
        "    - Ainsi, si la phrase est \"J'aime la nourriture\", modifiez la pour qu'elle soit \"<s><s> J'aime la nourriture\".\n",
        "    - Préparez également la phrase pour le comptage en ajoutant un jeton de fin \"<e>\" afin que le modèle puisse prédire quand terminer une phrase.\n",
        "\n",
        "Note technique : dans cette implémentation, vous stockerez les comptages sous forme de dictionnaire.\n",
        "\n",
        "    - La clé de chaque paire clé-valeur dans le dictionnaire est un tuple de n mots (et non une liste)\n",
        "    - La valeur dans la paire clé-valeur est le nombre d'occurrences.\n",
        "    - La raison pour laquelle on utilise un tuple comme clé au lieu d'une liste est qu'une liste en Python est un objet mutable (elle peut être modifiée après sa création). Un tuple est \"immuable\", c'est-à-dire qu'il ne peut pas être modifié après sa création. Un tuple peut donc être utilisé comme type de données pour la clé d'un dictionnaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAiE2KOj9fti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aca9098-14df-4bf2-ce4f-3e5b3060bad1"
      },
      "source": [
        "print(\"Uni-gram:\")\n",
        "print(count_n_grams(d3_net, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_n_grams(d3_net, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uni-gram:\n",
            "{('existait',): 2, ('truc',): 4, ('faire…',): 1, ('fauteuil',): 3, ('électrique',): 1, ('détection',): 1, ('danger',): 1, ('militer',): 1, ('langage',): 1, ('signes',): 1, ('ad...',): 1, ('ouvrir',): 1, ('porte',): 1, ('placar',): 1, ('telecomm...',): 1, ('faut',): 1, ('décomposer',): 1, ('geste',): 1, ('construit',): 1, ('d...',): 1, ('attacher',): 3, ('véhicule',): 1, ('...',): 4, ('transforme',): 1, ('run',): 1, ('roulant',): 1, ('fauteu...',): 2, ('déambulateur',): 1, ('performant',): 1, ('pliable',): 1, ('aider',): 2, ('alerter',): 1, ('automatiquement',): 1, ('fourni...',): 1, ('système',): 1, ('permettre',): 1, ('lecture',): 1, ('lit',): 1, ('(o...',): 1, ('pouvoir',): 1, ('appeler',): 1, ('téléphon...',): 1, (\"s'il\",): 1, ('contorsioner',): 1, ('parcs',): 1, ('jeux',): 1, ('adaptés',): 1, ('!!!',): 1, ('plages',): 1, ('domotique',): 1, ('fermer',): 1, ('domicile?',): 1, ('solutions',): 1, ('correctement',): 1, ('mo...',): 1, ('www.changing-places.org',): 1, ('voir',): 1, ('favoriser',): 1, ('hand',): 1, ('sport',): 1, ('petit.',): 1, ('lew',): 1, ('parents',): 1, ('pouvait',): 1, ('aid...',): 1, ('oui',): 1, ('enlever',): 1, ('spasticité',): 1, ('membres',): 1, ('inférieu...',): 1}\n",
            "Bi-gram:\n",
            "{('existait', 'truc'): 2, ('truc', 'faire…'): 1, ('faire…', 'fauteuil'): 1, ('fauteuil', 'électrique'): 1, ('électrique', 'détection'): 1, ('détection', 'danger'): 1, ('danger', 'militer'): 1, ('militer', 'langage'): 1, ('langage', 'signes'): 1, ('signes', 'ad...'): 1, ('ad...', 'ouvrir'): 1, ('ouvrir', 'porte'): 1, ('porte', 'placar'): 1, ('placar', 'telecomm...'): 1, ('telecomm...', 'faut'): 1, ('faut', 'décomposer'): 1, ('décomposer', 'geste'): 1, ('geste', 'construit'): 1, ('construit', 'd...'): 1, ('d...', 'attacher'): 1, ('attacher', 'fauteuil'): 1, ('fauteuil', 'véhicule'): 1, ('véhicule', '...'): 1, ('...', 'transforme'): 1, ('transforme', 'run'): 1, ('run', 'fauteuil'): 1, ('fauteuil', 'roulant'): 1, ('roulant', 'fauteu...'): 1, ('fauteu...', 'déambulateur'): 1, ('déambulateur', 'performant'): 1, ('performant', 'pliable'): 1, ('pliable', 'aider'): 1, ('aider', '...'): 2, ('...', 'truc'): 1, ('truc', 'alerter'): 1, ('alerter', 'automatiquement'): 1, ('automatiquement', 'fourni...'): 1, ('fourni...', 'système'): 1, ('système', 'permettre'): 1, ('permettre', 'lecture'): 1, ('lecture', 'lit'): 1, ('lit', '(o...'): 1, ('(o...', 'truc'): 1, ('truc', 'pouvoir'): 1, ('pouvoir', 'appeler'): 1, ('appeler', 'téléphon...'): 1, ('téléphon...', \"s'il\"): 1, (\"s'il\", 'existait'): 1, ('truc', 'aider'): 1, ('...', 'contorsioner'): 1, ('contorsioner', 'attacher'): 1, ('attacher', 'fauteu...'): 1, ('fauteu...', 'parcs'): 1, ('parcs', 'jeux'): 1, ('jeux', 'adaptés'): 1, ('adaptés', '!!!'): 1, ('!!!', 'plages'): 1, ('plages', '...'): 1, ('...', 'domotique'): 1, ('domotique', 'fermer'): 1, ('fermer', 'domicile?'): 1, ('domicile?', 'solutions'): 1, ('solutions', 'attacher'): 1, ('attacher', 'correctement'): 1, ('correctement', 'mo...'): 1, ('mo...', 'www.changing-places.org'): 1, ('www.changing-places.org', 'voir'): 1, ('voir', 'favoriser'): 1, ('favoriser', 'hand'): 1, ('hand', 'sport'): 1, ('sport', 'petit.'): 1, ('petit.', 'lew'): 1, ('lew', 'parents'): 1, ('parents', 'pouvait'): 1, ('pouvait', 'aid...'): 1, ('aid...', 'oui'): 1, ('oui', 'enlever'): 1, ('enlever', 'spasticité'): 1, ('spasticité', 'membres'): 1, ('membres', 'inférieu...'): 1, ('inférieu...',): 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysU_gns-9ryQ"
      },
      "source": [
        "**Estimer la probabilité d'un mot target**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUoEYpZ994jY"
      },
      "source": [
        "Nous voulons estimer la probabilité d'un mot donné par rapport aux \"n\" mots précédents en utilisant le nombre de n-grammes.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
        "\n",
        "Cette formule ne fonctionne pas quand le compte d'un n-gramme est égal à zéro..\n",
        "- Supposons que nous rencontrions un n-gram qui ne figurait pas dans les données de formation.  \n",
        "- Alors, l'équation (2) ne peut pas être évaluée (elle devient zéro divisé par zéro).\n",
        "\n",
        "Une façon de traiter les comptes de zéros est d'ajouter un lissage k.  \n",
        "- Le K-smoothing ajoute une constante positive $k$ à chaque numérateur et $k \\times |V|$ au dénominateur, où $|V|$ est le nombre de mots du vocabulaire.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
        "\n",
        "\n",
        "Pour les n-grammes qui ont un compte de zéro, l'équation (3) devient $\\frac{1}{|V|}$.\n",
        "- Cela signifie que tout n-gramme ayant une valeur nulle a la même probabilité de $\\frac{1}{|V|}$.\n",
        "\n",
        "Définissez une fonction qui calcule l'estimation de la probabilité (3) à partir du nombre de n-grammes et d'une constante $k$.\n",
        "\n",
        "- La fonction prend dans un dictionnaire \"n_gram_counts\", où la clé est le n-gram et la valeur est le nombre de ce n-gram.\n",
        "- La fonction prend également un autre dictionnaire \"n_plus1_gram_counts\", que vous utiliserez pour trouver le compte du n-gram précédent plus le mot courant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT9NmK7u99F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d911df8e-4d05-4a90-da33-b5aa42303a58"
      },
      "source": [
        "#test\n",
        "unique_words = [d3_net[i] for i in range(0, 50)]\n",
        "\n",
        "unigram_counts = count_n_grams(d3_net, 1)\n",
        "bigram_counts = count_n_grams(d3_net, 2)\n",
        "tmp_prob = estimate_probability(d3_net[3], d3_net[2], unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\"La probabilité estimée du mot 'faire', compte tenu du n-gramme 'truc' précédent, est: {tmp_prob:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La probabilité estimée du mot 'faire', compte tenu du n-gramme 'truc' précédent, est: 0.0200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6d0IFyG8Xq8"
      },
      "source": [
        "**Estimer la probabilité de tous les mots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByjlXyY8gZS"
      },
      "source": [
        "La fonction définie ci-dessous fait une boucle sur tous les mots du vocabulaire pour calculer les probabilités de tous les mots possibles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LedbZmHB8iAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632c86c7-b3b1-44e4-a9bb-264ea5a5174b"
      },
      "source": [
        "\n",
        "#test\n",
        "unique_words = [d3_net[i] for i in range(0, 50)]\n",
        "unigram_counts = count_n_grams(d3_net, 1)\n",
        "bigram_counts = count_n_grams(d3_net, 2)\n",
        "prob_globale = estimate_probabilities(d3_net[2], unigram_counts, bigram_counts, unique_words, k=1)\n",
        "print(prob_globale)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'existait': 0.019230769230769232, 'truc': 0.019230769230769232, 'faire…': 0.019230769230769232, 'fauteuil': 0.019230769230769232, 'électrique': 0.019230769230769232, 'détection': 0.019230769230769232, 'danger': 0.019230769230769232, 'militer': 0.019230769230769232, 'langage': 0.019230769230769232, 'signes': 0.019230769230769232, 'ad...': 0.019230769230769232, 'ouvrir': 0.019230769230769232, 'porte': 0.019230769230769232, 'placar': 0.019230769230769232, 'telecomm...': 0.019230769230769232, 'faut': 0.019230769230769232, 'décomposer': 0.019230769230769232, 'geste': 0.019230769230769232, 'construit': 0.019230769230769232, 'd...': 0.019230769230769232, 'attacher': 0.019230769230769232, 'véhicule': 0.019230769230769232, '...': 0.019230769230769232, 'transforme': 0.019230769230769232, 'run': 0.019230769230769232, 'roulant': 0.019230769230769232, 'fauteu...': 0.019230769230769232, 'déambulateur': 0.019230769230769232, 'performant': 0.019230769230769232, 'pliable': 0.019230769230769232, 'aider': 0.019230769230769232, 'alerter': 0.019230769230769232, 'automatiquement': 0.019230769230769232, 'fourni...': 0.019230769230769232, 'système': 0.019230769230769232, 'permettre': 0.019230769230769232, 'lecture': 0.019230769230769232, 'lit': 0.019230769230769232, '(o...': 0.019230769230769232, 'pouvoir': 0.019230769230769232, 'appeler': 0.019230769230769232, 'téléphon...': 0.019230769230769232, \"s'il\": 0.019230769230769232, '<e>': 0.019230769230769232, '<oov>': 0.019230769230769232}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oPw7DNs-XkI"
      },
      "source": [
        "**Matrices de probabilités**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5KviM5i-vmM"
      },
      "source": [
        "Ce tableau ci-dessous présente la prémière matrice de probabilités que nous réalisons ici. Elle nous premet de voir quels peuvent être les mots corrélés dans notre texte. Elle nous montre aussi deux types de corrélation. Le premier est le type de mot qui doivent être associés à un autre pour avoir du sens, comme \"langage signes\". Le second type est une corrélation qui donnerait au mot plus d'impact ou des associations de mots qui transmettraient une idée comme \"militer langage\", \"pouvoir appeler\" ou encore \"détection danger\".\n",
        "\n",
        "Ce qui pourrait maintenant être fait est de séparer ces deux types et garder la seconde mais transformer la première. Il faudrait créer une liste d'expression qui pourrait revenir comme \"fauteuil roulant\" ou \"langage signes\", qui pourraient être remplacer par un mot équivalent. Cela apporterait un nettoyage dans cette matrices et nous permettrait de sans doute observer des thèmes ou extraire des idées du textes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENiF7EuR-eTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "f75dd419-396a-4899-b4a2-1760be4ed8c8"
      },
      "source": [
        "unique_words = [d3_net[i] for i in range(0, 70)]\n",
        "bigram_counts = count_n_grams(d3_net, 2)\n",
        "\n",
        "print('bigram counts')\n",
        "display(make_count_matrix(bigram_counts, unique_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bigram counts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>faire…</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>électrique</th>\n",
              "      <th>détection</th>\n",
              "      <th>danger</th>\n",
              "      <th>militer</th>\n",
              "      <th>langage</th>\n",
              "      <th>signes</th>\n",
              "      <th>ad...</th>\n",
              "      <th>ouvrir</th>\n",
              "      <th>porte</th>\n",
              "      <th>placar</th>\n",
              "      <th>telecomm...</th>\n",
              "      <th>faut</th>\n",
              "      <th>décomposer</th>\n",
              "      <th>geste</th>\n",
              "      <th>construit</th>\n",
              "      <th>d...</th>\n",
              "      <th>attacher</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>véhicule</th>\n",
              "      <th>...</th>\n",
              "      <th>transforme</th>\n",
              "      <th>run</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>roulant</th>\n",
              "      <th>fauteu...</th>\n",
              "      <th>déambulateur</th>\n",
              "      <th>performant</th>\n",
              "      <th>pliable</th>\n",
              "      <th>aider</th>\n",
              "      <th>...</th>\n",
              "      <th>truc</th>\n",
              "      <th>alerter</th>\n",
              "      <th>automatiquement</th>\n",
              "      <th>fourni...</th>\n",
              "      <th>système</th>\n",
              "      <th>permettre</th>\n",
              "      <th>lecture</th>\n",
              "      <th>lit</th>\n",
              "      <th>(o...</th>\n",
              "      <th>truc</th>\n",
              "      <th>pouvoir</th>\n",
              "      <th>appeler</th>\n",
              "      <th>téléphon...</th>\n",
              "      <th>s'il</th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>aider</th>\n",
              "      <th>...</th>\n",
              "      <th>contorsioner</th>\n",
              "      <th>attacher</th>\n",
              "      <th>fauteu...</th>\n",
              "      <th>parcs</th>\n",
              "      <th>jeux</th>\n",
              "      <th>adaptés</th>\n",
              "      <th>!!!</th>\n",
              "      <th>plages</th>\n",
              "      <th>...</th>\n",
              "      <th>domotique</th>\n",
              "      <th>fermer</th>\n",
              "      <th>domicile?</th>\n",
              "      <th>solutions</th>\n",
              "      <th>attacher</th>\n",
              "      <th>correctement</th>\n",
              "      <th>mo...</th>\n",
              "      <th>www.changing-places.org</th>\n",
              "      <th>voir</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;oov&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(aider,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(système,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(construit,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(domotique,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(correctement,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(déambulateur,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>()</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(faut,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(attacher,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(téléphon...,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 72 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 existait  truc  faire…  ...  voir  <e>  <oov>\n",
              "(aider,)              0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(système,)            0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(construit,)          0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(domotique,)          0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(correctement,)       0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "...                   ...   ...     ...  ...   ...  ...    ...\n",
              "(déambulateur,)       0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "()                    0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(faut,)               0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(attacher,)           0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(téléphon...,)        0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "\n",
              "[70 rows x 72 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nrw3OPpMMaF"
      },
      "source": [
        "Les matrices suivantes sont d'autres matrices de probabilités. Elles montrent la probabilité des mots du texte en fonction des autres mots. La première est une matrice de probabilité effectuant la fonctionalité bi-gram soit par couple de mots. Elle nous montre la probabilité d'un mot par rapport à un autre. Par exmple, si il existe le mot \"véhicule\" dans le texte, en fonction de ce texte, la probabilité que il y ait aussi \"existait\" est de 0.018868. \n",
        "\n",
        "La seconde matrice utilise les fonctionalités du trigram. Cela signifie que par couple de mots, il matrice calculera la probabilité d'un mot. Par exemple ici, la matrice choisit un couple de mots du texte (ouvrir, porte). Ces mots semblent très corrélés. Elle montre ensuite une probabilité importante de l'existance du mot \"placar\" soit 0.0377. Cela peut être interpréter par le fait que la matrice trouve des mots corrélés qui peuvent faire partis d'un thème particulier, comme ici le fait d'ouvrir une porte. Puis elle préduit qu'il ait une importante probabilité que d'autres mots similaires ou appartenant au même thème apparaissent. Le mot \"placar\" peut être simialire ici, au mot \"porte\" car il nécessite la même action pour l'utiliser soit le fait d'ouvrir. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ1VGEvDKV1s"
      },
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, vocabulary)\n",
        "    count_matrix += k\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KnO18RB-6F0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "d5115a9f-eb17-4c09-a2b3-cb8da8fad1eb"
      },
      "source": [
        "unique_words = [d3_net[i] for i in range(0, 50)]\n",
        "bigram_counts = count_n_grams(d3_net, 2)\n",
        "print(\"bigram probabilities\")\n",
        "#print(make_probability_matrix(bigram_counts, unique_words, k=1))\n",
        "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bigram probabilities\n",
            "2021-01-06 21:26:30,460 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>faire…</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>électrique</th>\n",
              "      <th>détection</th>\n",
              "      <th>danger</th>\n",
              "      <th>militer</th>\n",
              "      <th>langage</th>\n",
              "      <th>signes</th>\n",
              "      <th>ad...</th>\n",
              "      <th>ouvrir</th>\n",
              "      <th>porte</th>\n",
              "      <th>placar</th>\n",
              "      <th>telecomm...</th>\n",
              "      <th>faut</th>\n",
              "      <th>décomposer</th>\n",
              "      <th>geste</th>\n",
              "      <th>construit</th>\n",
              "      <th>d...</th>\n",
              "      <th>attacher</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>véhicule</th>\n",
              "      <th>...</th>\n",
              "      <th>transforme</th>\n",
              "      <th>run</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>roulant</th>\n",
              "      <th>fauteu...</th>\n",
              "      <th>déambulateur</th>\n",
              "      <th>performant</th>\n",
              "      <th>pliable</th>\n",
              "      <th>aider</th>\n",
              "      <th>...</th>\n",
              "      <th>truc</th>\n",
              "      <th>alerter</th>\n",
              "      <th>automatiquement</th>\n",
              "      <th>fourni...</th>\n",
              "      <th>système</th>\n",
              "      <th>permettre</th>\n",
              "      <th>lecture</th>\n",
              "      <th>lit</th>\n",
              "      <th>(o...</th>\n",
              "      <th>truc</th>\n",
              "      <th>pouvoir</th>\n",
              "      <th>appeler</th>\n",
              "      <th>téléphon...</th>\n",
              "      <th>s'il</th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;oov&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(aider,)</th>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(système,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(construit,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(domotique,)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(correctement,)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(déambulateur,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>()</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(faut,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(attacher,)</th>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(téléphon...,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 existait      truc    faire…  ...      truc       <e>     <oov>\n",
              "(aider,)         0.018519  0.018519  0.018519  ...  0.018519  0.018519  0.018519\n",
              "(système,)       0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "(construit,)     0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "(domotique,)     0.019231  0.019231  0.019231  ...  0.019231  0.019231  0.019231\n",
              "(correctement,)  0.019231  0.019231  0.019231  ...  0.019231  0.019231  0.019231\n",
              "...                   ...       ...       ...  ...       ...       ...       ...\n",
              "(déambulateur,)  0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "()               0.019231  0.019231  0.019231  ...  0.019231  0.019231  0.019231\n",
              "(faut,)          0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "(attacher,)      0.018519  0.018519  0.018519  ...  0.018519  0.018519  0.018519\n",
              "(téléphon...,)   0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "\n",
              "[70 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZyBw-NY_AbV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "c7ea0fb8-61dd-42cb-f700-e59b827a3ba3"
      },
      "source": [
        "print(\"trigram probabilities\")\n",
        "trigram_counts = count_n_grams(d3_net, 3)\n",
        "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trigram probabilities\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>faire…</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>électrique</th>\n",
              "      <th>détection</th>\n",
              "      <th>danger</th>\n",
              "      <th>militer</th>\n",
              "      <th>langage</th>\n",
              "      <th>signes</th>\n",
              "      <th>ad...</th>\n",
              "      <th>ouvrir</th>\n",
              "      <th>porte</th>\n",
              "      <th>placar</th>\n",
              "      <th>telecomm...</th>\n",
              "      <th>faut</th>\n",
              "      <th>décomposer</th>\n",
              "      <th>geste</th>\n",
              "      <th>construit</th>\n",
              "      <th>d...</th>\n",
              "      <th>attacher</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>véhicule</th>\n",
              "      <th>...</th>\n",
              "      <th>transforme</th>\n",
              "      <th>run</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>roulant</th>\n",
              "      <th>fauteu...</th>\n",
              "      <th>déambulateur</th>\n",
              "      <th>performant</th>\n",
              "      <th>pliable</th>\n",
              "      <th>aider</th>\n",
              "      <th>...</th>\n",
              "      <th>truc</th>\n",
              "      <th>alerter</th>\n",
              "      <th>automatiquement</th>\n",
              "      <th>fourni...</th>\n",
              "      <th>système</th>\n",
              "      <th>permettre</th>\n",
              "      <th>lecture</th>\n",
              "      <th>lit</th>\n",
              "      <th>(o...</th>\n",
              "      <th>truc</th>\n",
              "      <th>pouvoir</th>\n",
              "      <th>appeler</th>\n",
              "      <th>téléphon...</th>\n",
              "      <th>s'il</th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;oov&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(jeux, adaptés)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(téléphon..., s'il)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(..., domotique)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(faut, décomposer)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(lecture, lit)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(ouvrir, porte)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(fauteu..., parcs)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(enlever, spasticité)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(plages, ...)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(telecomm..., faut)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       existait      truc  ...       <e>     <oov>\n",
              "(jeux, adaptés)        0.019231  0.019231  ...  0.019231  0.019231\n",
              "(téléphon..., s'il)    0.018868  0.018868  ...  0.018868  0.018868\n",
              "(..., domotique)       0.019231  0.019231  ...  0.019231  0.019231\n",
              "(faut, décomposer)     0.018868  0.018868  ...  0.018868  0.018868\n",
              "(lecture, lit)         0.018868  0.018868  ...  0.018868  0.018868\n",
              "...                         ...       ...  ...       ...       ...\n",
              "(ouvrir, porte)        0.018868  0.018868  ...  0.018868  0.018868\n",
              "(fauteu..., parcs)     0.019231  0.019231  ...  0.019231  0.019231\n",
              "(enlever, spasticité)  0.019231  0.019231  ...  0.019231  0.019231\n",
              "(plages, ...)          0.019231  0.019231  ...  0.019231  0.019231\n",
              "(telecomm..., faut)    0.018868  0.018868  ...  0.018868  0.018868\n",
              "\n",
              "[81 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1BvjTRGn-K8"
      },
      "source": [
        "La prochaine étape serait de garder dans un nouveau tableau les mots reliés à une probabilité supérieur à 0.030. Certains besoins ou thèmes vont pouvoir apparaitre. Cela aidera pour la prochaine étape de clustering puis de l'élaboration de l'algorithm de machine learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_D60dslzfhQ"
      },
      "source": [
        "# Lemmatization et racine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uK2ynCXlys"
      },
      "source": [
        "Voic un des résultats de lemmatization. Cette fonction ci-dessous nous donne une lemmatization très simplifiée. Elle n'est pas la plus efficace. Nous conseillons l'utilisation des librairies **spaCy**. Ici, nous ne utilisons pas mais potentiellement elles serait plus intéressante car elle est travaillée pour différentes langues . Elle ne s'applique pas uniquement pour l'anglais mais aussi pour des textes français ou encore allemand.\n",
        "\n",
        "Cependant, les résultats que nous obtenons sont satisfaisants.Une grande partie des mots sont ramenés à leur racine. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0EH1ndSK2uV"
      },
      "source": [
        "def lem(data): \n",
        "\n",
        "  # stemming \n",
        "  porter_stemmer = nlp.PorterStemmer()\n",
        "  roots = [porter_stemmer.stem(each) for each in d3_net]\n",
        "  print(\"result of stemming: \",roots)\n",
        "\n",
        "  # lemmatization \n",
        "  lemma = nlp.WordNetLemmatizer()\n",
        "  lem_roots = [lemma.lemmatize(each) for each in roots]\n",
        "  print(\"result of lemmatization: \",lem_roots)\n",
        "\n",
        "  return lem_roots \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LogxAjOiLShF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a3baa4-66bc-4bf4-c252-1c2889582666"
      },
      "source": [
        "'''d1_lem = lem(d1_net)\n",
        "d2_lem = lem(d2_net)\n",
        "d3_lem = lem(d3_net)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'d1_lem = lem(d1_net)\\nd2_lem = lem(d2_net)\\nd3_lem = lem(d3_net)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rROBNuykOxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034c5cba-6d11-4cd8-ceac-11b0d779a4dc"
      },
      "source": [
        "d3_lem = lem(d3_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result of stemming:  ['existait', 'truc', 'faire…', 'fauteuil', 'électriqu', 'détection', 'danger', 'milit', 'langag', 'sign', 'ad...', 'ouvrir', 'port', 'placar', 'telecomm...', 'faut', 'décompos', 'gest', 'construit', 'd...', 'attach', 'fauteuil', 'véhicul', '...', 'transform', 'run', 'fauteuil', 'roulant', 'fauteu...', 'déambulateur', 'perform', 'pliabl', 'aider', '...', 'truc', 'alert', 'automatiqu', 'fourni...', 'systèm', 'permettr', 'lectur', 'lit', '(o...', 'truc', 'pouvoir', 'appel', 'téléphon...', \"s'il\", 'existait', 'truc', 'aider', '...', 'contorsion', 'attach', 'fauteu...', 'parc', 'jeux', 'adapté', '!!!', 'plage', '...', 'domotiqu', 'fermer', 'domicile?', 'solut', 'attach', 'correct', 'mo...', 'www.changing-places.org', 'voir', 'favoris', 'hand', 'sport', 'petit.', 'lew', 'parent', 'pouvait', 'aid...', 'oui', 'enlev', 'spasticité', 'membr', 'inférieu...']\n",
            "result of lemmatization:  ['existait', 'truc', 'faire…', 'fauteuil', 'électriqu', 'détection', 'danger', 'milit', 'langag', 'sign', 'ad...', 'ouvrir', 'port', 'placar', 'telecomm...', 'faut', 'décompos', 'gest', 'construit', 'd...', 'attach', 'fauteuil', 'véhicul', '...', 'transform', 'run', 'fauteuil', 'roulant', 'fauteu...', 'déambulateur', 'perform', 'pliabl', 'aider', '...', 'truc', 'alert', 'automatiqu', 'fourni...', 'systèm', 'permettr', 'lectur', 'lit', '(o...', 'truc', 'pouvoir', 'appel', 'téléphon...', \"s'il\", 'existait', 'truc', 'aider', '...', 'contorsion', 'attach', 'fauteu...', 'parc', 'jeux', 'adapté', '!!!', 'plage', '...', 'domotiqu', 'fermer', 'domicile?', 'solut', 'attach', 'correct', 'mo...', 'www.changing-places.org', 'voir', 'favoris', 'hand', 'sport', 'petit.', 'lew', 'parent', 'pouvait', 'aid...', 'oui', 'enlev', 'spasticité', 'membr', 'inférieu...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfI6Twc9MrNr"
      },
      "source": [
        "# Clustering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjvECIr8zskI"
      },
      "source": [
        "## Trouver des réponses similaires "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz955hAYVKrv"
      },
      "source": [
        "### Première méthode : Nearest Neighbor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBBy0rmSl2P9"
      },
      "source": [
        "Dans cette section, le but est trouvé des réponses similaires et de les rassembler ensembles. Nous allons introduire un document comprenant les \"embeddings\", soit le chiffrement des mots. Nous allons les utilisés pour assimilés les mots de notre dataset avec un chiffrement qui sera lui-même utilisé pour vectoriser ces mots. \n",
        "\n",
        "Ces vecteurs vont être comparés et ainsi, analysés pour ainsi les rassembler en groupe. Grâce à cela, nous pourrons comment à classifier les types de réponses et nettoyer encore plus profondemment notre dataset. \n",
        "\n",
        "Cette étape est notre premier étape de clustering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44HuXWApnFhI"
      },
      "source": [
        "Nous importons un document qui contient les équivalents chiffrés de chaque mot de notre dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fxLtV58k350"
      },
      "source": [
        "fr_embeddings = os.path.join('/content/fr_embeddings.p')\n",
        "fr_embeddings = str(fr_embeddings)\n",
        "d3_lem = str(d3_lem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r12RjiVvnZgd"
      },
      "source": [
        "Dans un premier temps, nous allons associer aux mots présents dans notre texte des embeddings, puis nous allons vectoriser ces mots. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNibX1i6n-FE"
      },
      "source": [
        "def mot_vecteur(data, fr_embeddings): \n",
        "\n",
        "  # associer chiffrement aux mots \n",
        "  mot_embedding = get_document_embedding(data, fr_embeddings)\n",
        "  #print('Mots chiffrés:', mot_embedding)\n",
        "\n",
        "  # Vectoriser les mots \n",
        "  mot_vect, ind2data = get_document_vecs(data, fr_embeddings)\n",
        "  #print('Mots vectorisés:', mot_vect, ind2data)\n",
        "\n",
        "  return mot_embedding, mot_vect, ind2data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnANHZRjo8oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c53a91-bc93-4f37-fafb-07ccbde1dfa5"
      },
      "source": [
        "mot_embedding, mot_vect, ind2data = mot_vecteur(d3_lem, fr_embeddings)\n",
        "print(mot_vect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy0745_GkFZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c719df-8ae1-4661-a8eb-6a4894599250"
      },
      "source": [
        "print(f\"length of dictionary {len(ind2data)}\")\n",
        "print(f\"shape of document_vecs {mot_vect.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of dictionary 859\n",
            "shape of document_vecs (859, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DLkiHMFx4lz"
      },
      "source": [
        "Maintenant que nous avons créer ces vecteurs, nous voulons assembler entre elles les réponses les plus similaires. \n",
        "\n",
        "Pour cela, nous allons créer des tables de hachages et appeler les fonctions de clustering comme K-means et K-neighboors. Ces fonctions vont permettre de rassembler les réponses les plus simalaires grâce à leur vecteurs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-2guHzpkJtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d118c7e6-abfb-4c28-90b5-0dea7aefebc4"
      },
      "source": [
        "# this gives you a similar tweet as your input.\n",
        "# this implementation is vectorized...\n",
        "import numpy as np\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "idx = np.argmax(cosine_similarity(mot_vect, mot_embedding))\n",
        "print(d3_lem[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTe7TsgikMU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5060171-dae1-4506-f630-28e0dfa59b60"
      },
      "source": [
        "N_VECS = len(d3_lem)       # This many vectors.\n",
        "N_DIMS = len(ind2data[1])     # Vector dimensionality.\n",
        "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of vectors is 859 and each has 300 dimensions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdrHqpcOkaHH"
      },
      "source": [
        "# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n",
        "N_PLANES = 10\n",
        "# Number of times to repeat the hashing to improve the search.\n",
        "N_UNIVERSES = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAF8TpzOkb28"
      },
      "source": [
        "np.random.seed(0)\n",
        "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
        "            for _ in range(N_UNIVERSES)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaJrVJFA0PAA"
      },
      "source": [
        "Nous appliquons donc les fonctions suivantes :\n",
        "\n",
        "* la table de hachage \n",
        "* l'approximation knn \n",
        "* le vecteur le plus proche "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1n8LPDXf1KW"
      },
      "source": [
        "np.random.seed(0)\n",
        "idx = 0\n",
        "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
        "vec = np.random.rand(1, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gTX34K01IL0"
      },
      "source": [
        "def table_hash(vec,document_vecs, planes) : \n",
        "\n",
        "  print(f\" The hash value for this vector,\",\n",
        "      f\"and the set of planes at index {idx},\",\n",
        "      f\"is {hash_value_of_vector(vec, planes)}\")\n",
        "  \n",
        "  tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
        "  print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
        "  print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
        "  print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")\n",
        "\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LGEGCMo2J4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d479cfa-db04-4852-ad6a-50f637b49841"
      },
      "source": [
        "hash = table_hash(vec, mot_vect, planes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The hash value for this vector, and the set of planes at index 0, is 768\n",
            "The hash table at key 0 has 859 document vectors\n",
            "The id table at key 0 has 859\n",
            "The first 5 document indices stored at key 0 of are [0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKJlFiUnlgzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ae3ab9-03ca-4f70-b351-cd5265b04fb0"
      },
      "source": [
        "# Creating the hashtables\n",
        "hash_tables = []\n",
        "id_tables = []\n",
        "for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n",
        "    print('working on hash universe #:', universe_id)\n",
        "    planes = planes_l[universe_id]\n",
        "    hash_table, id_table = make_hash_table(vec, planes)\n",
        "    hash_tables.append(hash_table)\n",
        "    id_tables.append(id_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "working on hash universe #: 0\n",
            "working on hash universe #: 1\n",
            "working on hash universe #: 2\n",
            "working on hash universe #: 3\n",
            "working on hash universe #: 4\n",
            "working on hash universe #: 5\n",
            "working on hash universe #: 6\n",
            "working on hash universe #: 7\n",
            "working on hash universe #: 8\n",
            "working on hash universe #: 9\n",
            "working on hash universe #: 10\n",
            "working on hash universe #: 11\n",
            "working on hash universe #: 12\n",
            "working on hash universe #: 13\n",
            "working on hash universe #: 14\n",
            "working on hash universe #: 15\n",
            "working on hash universe #: 16\n",
            "working on hash universe #: 17\n",
            "working on hash universe #: 18\n",
            "working on hash universe #: 19\n",
            "working on hash universe #: 20\n",
            "working on hash universe #: 21\n",
            "working on hash universe #: 22\n",
            "working on hash universe #: 23\n",
            "working on hash universe #: 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3xE_cGRfnRv"
      },
      "source": [
        "#document_vecs, ind2Tweet\n",
        "doc_id = 0\n",
        "doc_to_search = d3_lem[doc_id]\n",
        "#vec_to_search = mot_vect[doc_id]\n",
        "vec_to_search = ind2data[doc_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnHMTpwfif_e"
      },
      "source": [
        "Nous cherchons maintenant les voisins les plus proches de chaque vecteurs. Ces vecteurs sont chacune des réponses des intervenants. Nous souhaitons trouver des réponses similaires pour ainsi les rassembler et les traiter chacune différemment. Cela nous permettra d'avoir des clusters et de les analyze chacun afin d'en extraire des besoins. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT4fWhY_lk4q"
      },
      "source": [
        "# This is the code used to do the fast nearest neighbor search. Feel free to go over it\n",
        "def approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
        "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
        "    assert num_universes_to_use <= N_UNIVERSES\n",
        "\n",
        "    # Vectors that will be checked as possible nearest neighbor\n",
        "    vecs_to_consider_l = list()\n",
        "\n",
        "    # list of document IDs\n",
        "    ids_to_consider_l = list()\n",
        "\n",
        "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
        "    ids_to_consider_set = set()\n",
        "\n",
        "    # loop through the universes of planes\n",
        "    for universe_id in range(num_universes_to_use):\n",
        "\n",
        "        # get the set of planes from the planes_l list, for this particular universe_id\n",
        "        planes = planes_l[universe_id]\n",
        "\n",
        "        # get the hash value of the vector for this set of planes\n",
        "        hash_value = hash_value_of_vector(v, planes)\n",
        "\n",
        "        # get the hash table for this particular universe_id\n",
        "        hash_table = hash_tables[universe_id]\n",
        "\n",
        "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
        "        document_vectors_l = hash_table[hash_value]\n",
        "\n",
        "        # get the id_table for this particular universe_id\n",
        "        id_table = id_tables[universe_id]\n",
        "\n",
        "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
        "        new_ids_to_consider = id_table[hash_value]\n",
        "\n",
        "        # remove the id of the document that we're searching\n",
        "        if doc_id in new_ids_to_consider:\n",
        "            new_ids_to_consider.remove(doc_id)\n",
        "            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
        "\n",
        "        # loop through the subset of document vectors to consider\n",
        "        for i, new_id in enumerate(new_ids_to_consider):\n",
        "\n",
        "            # if the document ID is not yet in the set ids_to_consider...\n",
        "            if new_id not in ids_to_consider_set:\n",
        "                # access document_vectors_l list at index i to get the embedding\n",
        "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
        "                document_vector_at_i = document_vectors_l[i]\n",
        "                vecs_to_consider_l.append(document_vector_at_i)\n",
        "\n",
        "                # append the new_id (the index for the document) to the list of ids to consider\n",
        "                ids_to_consider_l.append(new_id)\n",
        "\n",
        "                # also add the new_id to the set of ids to consider\n",
        "                # (use this to check if new_id is not already in the IDs to consider)\n",
        "                ids_to_consider_set.add(new_id)\n",
        "\n",
        "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
        "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
        "\n",
        "    # convert the vecs to consider set to a list, then to a numpy array\n",
        "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
        "\n",
        "    # call nearest neighbors on the reduced list of candidate vectors\n",
        "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
        "\n",
        "    # Use the nearest neighbor index list as indices into the ids to consider\n",
        "    # create a list of nearest neighbors by the document ids\n",
        "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
        "                            for idx in nearest_neighbor_idx_l]\n",
        "\n",
        "    return nearest_neighbor_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BO8FPSnienN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ff7c63-17ae-482e-f690-0a273c42cad4"
      },
      "source": [
        "# Fonction recherchant le voisin le plus proche \n",
        "nearest_neighbor_ids = approximate_knn(doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5) \n",
        "\n",
        "print(nearest_neighbor_ids)\n",
        "\n",
        "\n",
        "print(f\"Nearest neighbors for document {doc_id}\")\n",
        "print(f\"Document contents: {doc_to_search}\")\n",
        "print(\"\")\n",
        "\n",
        "for neighbor_id in nearest_neighbor_ids:\n",
        "  \n",
        "  print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
        "  print(f\"document contents: {d3_lem[neighbor_id]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fast considering 0 vecs\n",
            "[]\n",
            "Nearest neighbors for document 0\n",
            "Document contents: [\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dFgJOTEVTxd"
      },
      "source": [
        "### Seconde méthode : spaCy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtyC4gMkmnZ3"
      },
      "source": [
        "**Tokenisation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0O5bL6iVXAE"
      },
      "source": [
        "def return_token(sentence):\n",
        "    # Tokeniser la phrase\n",
        "    doc = nlp(sentence)\n",
        "    # Retourner le texte de chaque token\n",
        "    return [X.text for X in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "BKtTJe0umi_I",
        "outputId": "64d2ce1f-dfc8-4540-ff89-c2f680a82a42"
      },
      "source": [
        "return_token(cat3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-1ccc5ea3f888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreturn_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-07da97e8ea59>\u001b[0m in \u001b[0;36mreturn_token\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreturn_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Tokeniser la phrase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Retourner le texte de chaque token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGa9yDBNmqlc"
      },
      "source": [
        "**Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "6DpO0Xo8mmIy",
        "outputId": "db97ac8a-e96a-4e9b-a43c-9f4da55dae34"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('french'))\n",
        "\n",
        "clean_words = []\n",
        "for token in return_token(cat3):\n",
        "    if token not in stopWords:\n",
        "        clean_words.append(token)\n",
        "\n",
        "clean_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-7f9b9f5e1264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreturn_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopWords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mclean_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'return_token' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tw0aQ5447ym"
      },
      "source": [
        "## Trouver centroids "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYtSKuHIcPVb"
      },
      "source": [
        "Dans cette partie, un travail de recherche et d'essais est fait sur les fonctionnalités de Kmeans. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaKmmDmOfhEf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "7064345b-78df-4482-abe1-a8a3c87b1679"
      },
      "source": [
        "'''\n",
        "# Function: K Means\n",
        "# -------------\n",
        "# K-Means is an algorithm that takes in a dataset and a constant\n",
        "# k and returns k centroids (which define clusters of data in the\n",
        "# dataset which are similar to one another).\n",
        "def kmeans(dataSet, k):\n",
        "\t\n",
        "    # Initialize centroids randomly\n",
        "    numFeatures = dataSet.getNumFeatures()\n",
        "    centroids = getRandomCentroids(numFeatures, k)\n",
        "    \n",
        "    # Initialize book keeping vars.\n",
        "    iterations = 0\n",
        "    oldCentroids = None\n",
        "    \n",
        "    # Run the main k-means algorithm\n",
        "    while not shouldStop(oldCentroids, centroids, iterations):\n",
        "        # Save old centroids for convergence test. Book keeping.\n",
        "        oldCentroids = centroids\n",
        "        iterations += 1\n",
        "        \n",
        "        # Assign labels to each datapoint based on centroids\n",
        "        labels = getLabels(dataSet, centroids)\n",
        "        \n",
        "        # Assign centroids based on datapoint labels\n",
        "        centroids = getCentroids(dataSet, labels, k)\n",
        "        \n",
        "    # We can get the labels too by calling getLabels(dataSet, centroids)\n",
        "    return centroids\n",
        "    \n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Function: K Means\\n# -------------\\n# K-Means is an algorithm that takes in a dataset and a constant\\n# k and returns k centroids (which define clusters of data in the\\n# dataset which are similar to one another).\\ndef kmeans(dataSet, k):\\n\\t\\n    # Initialize centroids randomly\\n    numFeatures = dataSet.getNumFeatures()\\n    centroids = getRandomCentroids(numFeatures, k)\\n    \\n    # Initialize book keeping vars.\\n    iterations = 0\\n    oldCentroids = None\\n    \\n    # Run the main k-means algorithm\\n    while not shouldStop(oldCentroids, centroids, iterations):\\n        # Save old centroids for convergence test. Book keeping.\\n        oldCentroids = centroids\\n        iterations += 1\\n        \\n        # Assign labels to each datapoint based on centroids\\n        labels = getLabels(dataSet, centroids)\\n        \\n        # Assign centroids based on datapoint labels\\n        centroids = getCentroids(dataSet, labels, k)\\n        \\n    # We can get the labels too by calling getLabels(dataSet, centroids)\\n    return centroids\\n    \\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu254EvXfim7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8b8d350e-fb3f-493c-e650-e6dae816aee5"
      },
      "source": [
        "\"\"\"# Function: Should Stop\n",
        "# -------------\n",
        "# Returns True or False if k-means is done. K-means terminates either\n",
        "# because it has run a maximum number of iterations OR the centroids\n",
        "# stop changing.\n",
        "def shouldStop(oldCentroids, centroids, iterations):\n",
        "    if iterations > MAX_ITERATIONS: return True\n",
        "    return oldCentroids == centroids\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Function: Should Stop\\n# -------------\\n# Returns True or False if k-means is done. K-means terminates either\\n# because it has run a maximum number of iterations OR the centroids\\n# stop changing.\\ndef shouldStop(oldCentroids, centroids, iterations):\\n    if iterations > MAX_ITERATIONS: return True\\n    return oldCentroids == centroids\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1SlCSO1flZw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "739491bc-d1a1-4e22-eb43-85793b4f579e"
      },
      "source": [
        "\"\"\"# Function: Get Labels\n",
        "# -------------\n",
        "# Returns a label for each piece of data in the dataset. \n",
        "def getLabels(dataSet, centroids):\n",
        "  \n",
        "    # For each element in the dataset, chose the closest centroid. \n",
        "    # Make that centroid the element's label.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Function: Get Labels\\n# -------------\\n# Returns a label for each piece of data in the dataset. \\ndef getLabels(dataSet, centroids):\\n  \\n    # For each element in the dataset, chose the closest centroid. \\n    # Make that centroid the element's label.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjKL7X5efo9L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8e4daf44-27db-4499-cfee-b3cf647a7fb2"
      },
      "source": [
        "\"\"\"# Function: Get Centroids\n",
        "# -------------\n",
        "# Returns k random centroids, each of dimension n.\n",
        "def getCentroids(dataSet, labels, k):\n",
        "    # Each centroid is the geometric mean of the points that\n",
        "    # have that centroid's label. Important: If a centroid is empty (no points have\n",
        "    # that centroid's label) you should randomly re-initialize it.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Function: Get Centroids\\n# -------------\\n# Returns k random centroids, each of dimension n.\\ndef getCentroids(dataSet, labels, k):\\n    # Each centroid is the geometric mean of the points that\\n    # have that centroid's label. Important: If a centroid is empty (no points have\\n    # that centroid's label) you should randomly re-initialize it.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7dNlvi19SO3"
      },
      "source": [
        "# Répartition des données en train set et test set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6R725nstwdI"
      },
      "source": [
        "Le but final est de construire un algorithm capable de classifier chaque réponse de toutes les catégories de formulaires et d'en extraire un besoin. Nous voulons donc créer un algorithm de machine learning ou deep-learning qui serait capable de comprendre les besoins formulés par les intervenants. \n",
        "\n",
        "Nous réfléchissons à plusieurs manières d'élaborer un tel algorithm. \n",
        "\n",
        "Une option intéressante serait de relever manuellement quelques besoins récurrents et d'entrainer notre algorithm à les trouver dans notre texte. \n",
        "\n",
        "Une autre option serait de demander à l'algorithm de trouver ces besoins par lui-même grâce aux essais de clustering réalisés précedemment. Grâce aux vectorisations des mots, il pourrait ressembler les vecteurs les plus proches ou les plus similaires. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsO9iiffzlVK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "18ad766f-3355-446c-8937-2de846ec0862"
      },
      "source": [
        "'''#tokenized_data = tokenize_sentences(cat3_roots)\n",
        "#print(tokenized_data)\n",
        "random.seed(87)\n",
        "random.shuffle(d3_lem)\n",
        "\n",
        "train_size = int(len(d3_lem) * 0.8)\n",
        "train_data = d3_lem[0:train_size]\n",
        "test_data = d3_lem[train_size:]'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#tokenized_data = tokenize_sentences(cat3_roots)\\n#print(tokenized_data)\\nrandom.seed(87)\\nrandom.shuffle(d3_lem)\\n\\ntrain_size = int(len(d3_lem) * 0.8)\\ntrain_data = d3_lem[0:train_size]\\ntest_data = d3_lem[train_size:]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJqMbycK20Hy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "dc3a28cc-7ef3-498a-f569-f48d260b8f68"
      },
      "source": [
        "'''print(\"{} data are split into {} train and {} test set\".format(\n",
        "    len(d3_lem), len(train_data), len(test_data)))\n",
        "\n",
        "print(\"First training sample:\")\n",
        "print(train_data[0])\n",
        "print(train_data)\n",
        "      \n",
        "print(\"First test sample\")\n",
        "print(test_data[0])'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'print(\"{} data are split into {} train and {} test set\".format(\\n    len(d3_lem), len(train_data), len(test_data)))\\n\\nprint(\"First training sample:\")\\nprint(train_data[0])\\nprint(train_data)\\n      \\nprint(\"First test sample\")\\nprint(test_data[0])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqREuz_zpXk8"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geAwKUaXWSNz"
      },
      "source": []
    }
  ]
}