{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wQdJFK5Vy5hB",
        "bDiVi63d2N6Y",
        "c_D60dslzfhQ",
        "3tw0aQ5447ym",
        "o7dNlvi19SO3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQdJFK5Vy5hB"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MP9FeCwy8s1"
      },
      "source": [
        "Dans cet essai je vais essayer d'appliquer des fonctions pr√©sent√©es dans les cours de NLP. A la fin de ce notebook j'appliquerai les fonctions les plus int√©ressantes √† notre dataset et j'essaierai d'am√©liorer notre code. \n",
        "\n",
        "Dans ce jupyter notebook, nous allons importer les data puis les s√©parer en trois groupes. Nous avons les formulaires des personnes concern√©es, de leur personnels soignants ainsi que de leur entourage. Nous allons analyzer chaque cat√©gorie et comparer les r√©sultats.  \n",
        "\n",
        "La premi√®re √©tape d'analyze est le nettoyage des donn√©es. Cette √©tape est longue et elle est une priorit√© dans notre projet. Nous voulons extraire des besoins de textes o√π il ne reste uniquement les informations les plus importantes. \n",
        "\n",
        "La seconde √©tape est d'examiner les mots restant √† leur racine. Nous voulons √©purer ces mots. Nous appelons √ßa la lemmatization. \n",
        "\n",
        "La prochaine √©tape se veut de nous aider √† classifier les mots restants en les rassemblant en fonction de leur importance, leur sens et le sentiment que les phrases peuvent transmettre. Nous appelons √ßa le clustering. Nous allons mettre au point plusieurs diff√©rentes mani√®res de trouver des r√©sultats. Puis nous les comparerons. \n",
        "\n",
        "Les documents n√©cessaires pour faire tourner le jupyter notebook se trouvent dans le dossier Colab dans le projet git appel√© \"Idlys\". Il vous suffit de le t√©l√©charger puis d'aller dans la banderole √† gauche de votre √©cran sur Colab, de cliquer sur la quatri√®me cat√©gorie appel√©e \"Dossiers\" ou \"Files\", puis d'importer vos fichiers. Il est important qu'ils ne soient pas importer via \"sample_data\" ou d'un drive car cela modifierait le chemin d'importation et les fichiers ne pourront √™tre lu. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDiVi63d2N6Y"
      },
      "source": [
        "# Librairies & Importation des donn√©es "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21EMMNPcJexv"
      },
      "source": [
        "**Importation** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4IvsgtNI4EV"
      },
      "source": [
        "> Importation de base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjyAf01xI8eT"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import scipy\n",
        "import sklearn\n",
        "import pdb\n",
        "import pickle\n",
        "import string\n",
        "import time\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqKgEURII_Cv"
      },
      "source": [
        "> Importation n√©cessaire pour le NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ei7DfbC2QXr",
        "outputId": "b289da07-a173-4062-8359-024d0665add0"
      },
      "source": [
        "# lemmatizer spacy\n",
        "!pip install spacy-lefff\n",
        "import spacy\n",
        "from spacy_lefff import LefffLemmatizer, POSTagger\n",
        "\n",
        "\n",
        "# from nltk\n",
        "import nltk.corpus\n",
        "import nltk as nlp\n",
        "nltk.download('wordnet')\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "nltk.data.path.append('.')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# from gensim\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# from os \n",
        "from os import getcwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy-lefff in /usr/local/lib/python3.6/dist-packages (0.3.7)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-lefff) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (51.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.19.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-lefff) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-lefff) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-lefff) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-lefff) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-lefff) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->spacy-lefff) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-lefff) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-lefff) (3.4.0)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIze0e8LqknK",
        "outputId": "d3920bb1-2a20-49aa-b455-b4eab4b91fba"
      },
      "source": [
        "!python -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fr_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz#egg=fr_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m‚úî Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQqL0-8dUWE8"
      },
      "source": [
        "import spacy\n",
        "\n",
        "fr_spaCy = spacy.load(\"fr\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01WLlqJKgx3U"
      },
      "source": [
        "#!pip install urllib3==1.25.10\n",
        "\n",
        "#!pip install smart_open==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0EgWRlZJMKA"
      },
      "source": [
        "> Importation des fonctions cr√©es en annexe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGbnw8QKJc6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9309034f-2505-43f8-9504-d0cc75faa8c9"
      },
      "source": [
        "from nettoyage import stopwords\n",
        "from Frequence import count_words, get_words_with_nplus_frequency, count_n_grams, estimate_probability, estimate_probabilities\n",
        "from Pour_aller_plus_loin import make_count_matrix, make_probability_matrix\n",
        "from Neighboor import cosine_similarity, get_dict, get_document_embedding, get_document_vecs, hash_value_of_vector, make_hash_table, approximate_knn, nearest_neighbor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT90vhz-ei2E"
      },
      "source": [
        "**Concat√©ner les donn√©es et cr√©er trois dataframes en fonction des intervenants**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZExSdNjGh7i0"
      },
      "source": [
        "Nous importons nos donn√©es. Nous avons donc trois sortes de formulaires. \n",
        "\n",
        "Nous avons : \n",
        "\n",
        "> df1 : r√©ponses des personnes concern√©es \n",
        "\n",
        "> df2 : r√©ponses de leur personnel soigant \n",
        "\n",
        "> df3 : r√©ponses de leur entourage "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdkNPinS6PH6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "88530b17-5bb3-43f8-ec38-fd13a6603cff"
      },
      "source": [
        "# Readind the excel and turning it into a single string\n",
        "'''df1 = pd.read_excel('/content/sample_data/Copie-de-r√©ponses-proches-51-100.xlsx', header = None)\n",
        "df1.head()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"df1 = pd.read_excel('/content/sample_data/Copie-de-r√©ponses-proches-51-100.xlsx', header = None)\\ndf1.head()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8p8kY8YjvHn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1e63d225-2eed-4a93-d9e7-1864d33f2ee1"
      },
      "source": [
        "'''df2 = pd.read_excel('/content/sample_data/Copie-de-r√©ponses-proches-51-100.xlsx', header = None)\n",
        "df2.head()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"df2 = pd.read_excel('/content/sample_data/Copie-de-r√©ponses-proches-51-100.xlsx', header = None)\\ndf2.head()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njSXtCtHjxtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e45de4-785f-4c83-b56d-72b6b7a77fce"
      },
      "source": [
        "df3 = pd.read_excel('Copie-de-r√©ponses-proches-51-100.xlsx', header = None)\n",
        "print(df3.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0  ...                                                  6\n",
            "0  N¬∞  ...       Si seulement il existait un truc pour faire‚Ä¶\n",
            "1  51  ...    Un fauteuil √©lectrique avec d√©tection de danger\n",
            "2  52  ...  Militer pour que le langage des signes soit ad...\n",
            "3  53  ...  OUVRIR UNE PORTE D UN PLACAR AVEC UNE TELECOMM...\n",
            "4  58  ...  Il faut d√©composer chaque geste construit en d...\n",
            "\n",
            "[5 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wNy-7067Cem"
      },
      "source": [
        "**S√©lectionner les colonnes qui pr√©sentent des besoins** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XweXM2YcgTr3"
      },
      "source": [
        "Dans cette section nous s√©lectionnons les colonnes qui pourraient pr√©senter des besoins. \n",
        "\n",
        "Nous avons trois cat√©gories diff√©rentes de formulaires. Nous allons donc cr√©er trois datasets diff√©rents pour chaque cat√©gories contenant ces colonnes. \n",
        "\n",
        "> cat1 : cat√©gorie 1, personne concern√©e \n",
        "\n",
        "> cat2 : cat√©gorie 2, personnel soigant\n",
        "\n",
        "> cat3 : cat√©gorie 3, entourage de personne concern√©e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKDXSFD4bIjf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "afbdaef1-d66a-4df1-a8f4-efd347e2190b"
      },
      "source": [
        "'''cat1 = df1[6]\n",
        "cat1 =cat1.to_string(index=False)\n",
        "print(cat1)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cat1 = df1[6]\\ncat1 =cat1.to_string(index=False)\\nprint(cat1)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuuI5voA7AhM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fbe220eb-5716-42aa-d6a5-ae1455cbe2c1"
      },
      "source": [
        "'''cat2 = df2[6]\n",
        "cat2 =cat2.to_string(index=False)\n",
        "print(cat2)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cat2 = df2[6]\\ncat2 =cat2.to_string(index=False)\\nprint(cat2)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiwCwuy4hYBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6840103-6e0a-4089-9810-7aaeca783270"
      },
      "source": [
        "cat3 = df3[6]\n",
        "cat3 =cat3.to_string(index=False)\n",
        "print(cat3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Si seulement il existait un truc pour faire‚Ä¶\n",
            "   Un fauteuil √©lectrique avec d√©tection de danger\n",
            " Militer pour que le langage des signes soit ad...\n",
            " OUVRIR UNE PORTE D UN PLACAR AVEC UNE TELECOMM...\n",
            " Il faut d√©composer chaque geste construit en d...\n",
            "                                               NaN\n",
            " Attacher le fauteuil dans le v√©hicule sans se ...\n",
            " pour transforme run fauteuil roulant en fauteu...\n",
            " Un d√©ambulateur performant pliable pour aider ...\n",
            "                                               NaN\n",
            " Un truc pour alerter automatiquement le fourni...\n",
            " Un syst√®me pour permettre la lecture au lit (o...\n",
            " Un truc aussi pour pouvoir appeler au t√©l√©phon...\n",
            " S'il existait un truc pour aider une personne ...\n",
            " ne pas se contorsioner pour attacher le fauteu...\n",
            " Des parcs de jeux adapt√©s !!! DES plages avec ...\n",
            "                domotique pour fermer le domicile?\n",
            "                                               NaN\n",
            "                                                 X\n",
            " Pas de solutions pour attacher correctement mo...\n",
            "                                               NaN\n",
            "                           www.changing-places.org\n",
            "                                    Voir au dessus\n",
            "                                               NaN\n",
            "           Favoriser le hand sport d√®s tout petit.\n",
            " Si seulement lew parents pouvait etre plus aid...\n",
            " Oui enlever la Spasticit√© des membres inf√©rieu...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o3eLXvLe8KC"
      },
      "source": [
        "# Nettoyage "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChEBPq9fIIew"
      },
      "source": [
        "Nous cr√©ons une fonction qui permet de nettoyer notre dataset. Cette fonction appelle d'autres fonctions cr√©e dans d'autres fichiers.\n",
        "\n",
        "> Rappel : Ces fonctions se trouvent dans le fichier .py appel√© \"Nettoyage\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23e8KV1wNeED"
      },
      "source": [
        "def clean(data): \n",
        "\n",
        "  # bags of words \n",
        "  data_1 = data.split()\n",
        "  #print(data_1)\n",
        "\n",
        "  # minuscule\n",
        "  data_2 = [word.lower() for word in data_1]\n",
        "  #print(data_2)\n",
        "\n",
        "  # enlever tous les stopwords \n",
        "  data_3 = stopwords(data_2)\n",
        "  #print(data_3)\n",
        "\n",
        "  # enlever tous les \"nan\" du texte\n",
        "  data_4 = [x for x in data_3 if str(x) != 'nan']\n",
        "  #print(data_4)\n",
        "\n",
        "  return data_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "316HUzhhfQ1r"
      },
      "source": [
        "Nous appliquons notre fonction √† toutes les cat√©gories. Nous cr√©eons un nouveau data set pour chacune des cat√©gories. \n",
        "\n",
        "> d1_net : donn√©es nettoy√©es cat√©gorie 1 \n",
        "\n",
        "> d2_net : donn√©es nettoy√©es cat√©gorie 2\n",
        "\n",
        "> d3_net : donn√©es nettoy√©es cat√©gorie 3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PugpFl2uypPH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "315d59f8-27b6-418d-bbbc-1144014897a5"
      },
      "source": [
        "'''d1_net = clean(cat1)\n",
        "d2_net = clean(cat2)\n",
        "d3_net = clean(cat3)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'d1_net = clean(cat1)\\nd2_net = clean(cat2)\\nd3_net = clean(cat3)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1znu77g2_tvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e893dfa-a0d9-42d9-fc39-a4e04ac8b44b"
      },
      "source": [
        "d3_net = clean(cat3)\n",
        "print(d3_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['existait', 'truc', 'faire‚Ä¶', 'fauteuil', '√©lectrique', 'd√©tection', 'danger', 'militer', 'langage', 'signes', 'ad...', 'ouvrir', 'porte', 'placar', 'telecomm...', 'faut', 'd√©composer', 'geste', 'construit', 'd...', 'attacher', 'fauteuil', 'v√©hicule', '...', 'transforme', 'run', 'fauteuil', 'roulant', 'fauteu...', 'd√©ambulateur', 'performant', 'pliable', 'aider', '...', 'truc', 'alerter', 'automatiquement', 'fourni...', 'syst√®me', 'permettre', 'lecture', 'lit', '(o...', 'truc', 'pouvoir', 'appeler', 't√©l√©phon...', \"s'il\", 'existait', 'truc', 'aider', '...', 'contorsioner', 'attacher', 'fauteu...', 'parcs', 'jeux', 'adapt√©s', '!!!', 'plages', '...', 'domotique', 'fermer', 'domicile?', 'solutions', 'attacher', 'correctement', 'mo...', 'www.changing-places.org', 'voir', 'favoriser', 'hand', 'sport', 'petit.', 'lew', 'parents', 'pouvait', 'aid...', 'oui', 'enlever', 'spasticit√©', 'membres', 'inf√©rieu...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRAxPqUEl4VG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9c-oXZ8pcoC"
      },
      "source": [
        "# Fr√©quence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcj5EQ99ABBG"
      },
      "source": [
        "Dans cette section nous voulons faire une comparaison sur les diff√©rents types de mots pr√©sents en fonction de chaque cat√©gorie.\n",
        "\n",
        "Nous allons chercher √† savoir la fr√©quence des mots dans chaque texte.Nous pourrions savoir si certains mots reviennent plus que d'autres dans chacune des cat√©ogries ou par exemple si une cat√©gorie utilisent plus certains mots en particulier.\n",
        "\n",
        "Nous allons proc√©der en plusieurs √©tapes. Nous voulons tout d'abord d√©compter chaque mot puis calculer leur fr√©quence. Dans un second temps, nous allons utiliser des foncitonnalit√©s plus avanc√©es comme la N-grams o√π nous pourrions d√©compter chaque mot mais aussi des couples de mots et pouvoir mieux analyzer notre texte. \n",
        "\n",
        "Puis nous allons voir comment nous pouvons afficher la fr√©quence des mots dans notre dataset. Nous allons d'abord r√©fl√©chir √† un certain mot cl√©, une target et calculer sa fr√©quence. Puis nous allons g√©n√©rer cette op√©rations sur l'ensemble de notre dataset et calculer les probablit√©s de chacun des mots. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T74T7kUIlu2e"
      },
      "source": [
        "def frequence(data):\n",
        "\n",
        "   # count chaque mot\n",
        "   data_1 = count_words(data)\n",
        "   print(\"Compte:\", data_1)\n",
        "\n",
        "   # fr√©quence de chaque mot \n",
        "   data_2 = get_words_with_nplus_frequency(data, count_threshold=2)\n",
        "   print(\"Fr√©quence:\", data_2)\n",
        "\n",
        "   # N-grams\n",
        "   data_3 = count_n_grams(data, 1)\n",
        "   print(\"N-grams:\", data_3)\n",
        "   data_32 = count_n_grams(data, 2)\n",
        "   print(\"N-grams par couples de mots:\", data_32)\n",
        "\n",
        "   # calculation probability target\n",
        "   #data_4 = estimate_probability(data)\n",
        "   unique_words = [d3_net[i] for i in range(0, 50)] # on prend les 50 premi√®res lignes de notre dataset \n",
        "   data_4 = estimate_probability(\"faire\", \"truc\", data_3, data_32, len(unique_words), k=1)\n",
        "   print(\"Probabilit√© de la target:\", data_4)\n",
        "\n",
        "   # calcul probaility all words \n",
        "   unique_words = [d3_net[i] for i in range(0, 50)]\n",
        "   data_5 = estimate_probabilities(\"faire\", data_3, data_32, unique_words, k=1)\n",
        "   print(\"Probabilit√© de chacun des mots:\", data_5)\n",
        "\n",
        "   return \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFrciY4W_fGb"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8CEBAHE_eXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77dee0c-04b4-4a47-b05d-2f3b3ba043b0"
      },
      "source": [
        "Get_Freq = frequence(d3_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compte: {'existait': 2, 'truc': 4, 'faire‚Ä¶': 1, 'fauteuil': 3, '√©lectrique': 1, 'd√©tection': 1, 'danger': 1, 'militer': 1, 'langage': 1, 'signes': 1, 'ad...': 1, 'ouvrir': 1, 'porte': 1, 'placar': 1, 'telecomm...': 1, 'faut': 1, 'd√©composer': 1, 'geste': 1, 'construit': 1, 'd...': 1, 'attacher': 3, 'v√©hicule': 1, '...': 4, 'transforme': 1, 'run': 1, 'roulant': 1, 'fauteu...': 2, 'd√©ambulateur': 1, 'performant': 1, 'pliable': 1, 'aider': 2, 'alerter': 1, 'automatiquement': 1, 'fourni...': 1, 'syst√®me': 1, 'permettre': 1, 'lecture': 1, 'lit': 1, '(o...': 1, 'pouvoir': 1, 'appeler': 1, 't√©l√©phon...': 1, \"s'il\": 1, 'contorsioner': 1, 'parcs': 1, 'jeux': 1, 'adapt√©s': 1, '!!!': 1, 'plages': 1, 'domotique': 1, 'fermer': 1, 'domicile?': 1, 'solutions': 1, 'correctement': 1, 'mo...': 1, 'www.changing-places.org': 1, 'voir': 1, 'favoriser': 1, 'hand': 1, 'sport': 1, 'petit.': 1, 'lew': 1, 'parents': 1, 'pouvait': 1, 'aid...': 1, 'oui': 1, 'enlever': 1, 'spasticit√©': 1, 'membres': 1, 'inf√©rieu...': 1}\n",
            "Fr√©quence: ['existait', 2, 'truc', 4, 'fauteuil', 3, 'attacher', 3, '...', 4, 'fauteu...', 2, 'aider', 2]\n",
            "N-grams: {('existait',): 2, ('truc',): 4, ('faire‚Ä¶',): 1, ('fauteuil',): 3, ('√©lectrique',): 1, ('d√©tection',): 1, ('danger',): 1, ('militer',): 1, ('langage',): 1, ('signes',): 1, ('ad...',): 1, ('ouvrir',): 1, ('porte',): 1, ('placar',): 1, ('telecomm...',): 1, ('faut',): 1, ('d√©composer',): 1, ('geste',): 1, ('construit',): 1, ('d...',): 1, ('attacher',): 3, ('v√©hicule',): 1, ('...',): 4, ('transforme',): 1, ('run',): 1, ('roulant',): 1, ('fauteu...',): 2, ('d√©ambulateur',): 1, ('performant',): 1, ('pliable',): 1, ('aider',): 2, ('alerter',): 1, ('automatiquement',): 1, ('fourni...',): 1, ('syst√®me',): 1, ('permettre',): 1, ('lecture',): 1, ('lit',): 1, ('(o...',): 1, ('pouvoir',): 1, ('appeler',): 1, ('t√©l√©phon...',): 1, (\"s'il\",): 1, ('contorsioner',): 1, ('parcs',): 1, ('jeux',): 1, ('adapt√©s',): 1, ('!!!',): 1, ('plages',): 1, ('domotique',): 1, ('fermer',): 1, ('domicile?',): 1, ('solutions',): 1, ('correctement',): 1, ('mo...',): 1, ('www.changing-places.org',): 1, ('voir',): 1, ('favoriser',): 1, ('hand',): 1, ('sport',): 1, ('petit.',): 1, ('lew',): 1, ('parents',): 1, ('pouvait',): 1, ('aid...',): 1, ('oui',): 1, ('enlever',): 1, ('spasticit√©',): 1, ('membres',): 1, ('inf√©rieu...',): 1}\n",
            "N-grams par couples de mots: {('existait', 'truc'): 2, ('truc', 'faire‚Ä¶'): 1, ('faire‚Ä¶', 'fauteuil'): 1, ('fauteuil', '√©lectrique'): 1, ('√©lectrique', 'd√©tection'): 1, ('d√©tection', 'danger'): 1, ('danger', 'militer'): 1, ('militer', 'langage'): 1, ('langage', 'signes'): 1, ('signes', 'ad...'): 1, ('ad...', 'ouvrir'): 1, ('ouvrir', 'porte'): 1, ('porte', 'placar'): 1, ('placar', 'telecomm...'): 1, ('telecomm...', 'faut'): 1, ('faut', 'd√©composer'): 1, ('d√©composer', 'geste'): 1, ('geste', 'construit'): 1, ('construit', 'd...'): 1, ('d...', 'attacher'): 1, ('attacher', 'fauteuil'): 1, ('fauteuil', 'v√©hicule'): 1, ('v√©hicule', '...'): 1, ('...', 'transforme'): 1, ('transforme', 'run'): 1, ('run', 'fauteuil'): 1, ('fauteuil', 'roulant'): 1, ('roulant', 'fauteu...'): 1, ('fauteu...', 'd√©ambulateur'): 1, ('d√©ambulateur', 'performant'): 1, ('performant', 'pliable'): 1, ('pliable', 'aider'): 1, ('aider', '...'): 2, ('...', 'truc'): 1, ('truc', 'alerter'): 1, ('alerter', 'automatiquement'): 1, ('automatiquement', 'fourni...'): 1, ('fourni...', 'syst√®me'): 1, ('syst√®me', 'permettre'): 1, ('permettre', 'lecture'): 1, ('lecture', 'lit'): 1, ('lit', '(o...'): 1, ('(o...', 'truc'): 1, ('truc', 'pouvoir'): 1, ('pouvoir', 'appeler'): 1, ('appeler', 't√©l√©phon...'): 1, ('t√©l√©phon...', \"s'il\"): 1, (\"s'il\", 'existait'): 1, ('truc', 'aider'): 1, ('...', 'contorsioner'): 1, ('contorsioner', 'attacher'): 1, ('attacher', 'fauteu...'): 1, ('fauteu...', 'parcs'): 1, ('parcs', 'jeux'): 1, ('jeux', 'adapt√©s'): 1, ('adapt√©s', '!!!'): 1, ('!!!', 'plages'): 1, ('plages', '...'): 1, ('...', 'domotique'): 1, ('domotique', 'fermer'): 1, ('fermer', 'domicile?'): 1, ('domicile?', 'solutions'): 1, ('solutions', 'attacher'): 1, ('attacher', 'correctement'): 1, ('correctement', 'mo...'): 1, ('mo...', 'www.changing-places.org'): 1, ('www.changing-places.org', 'voir'): 1, ('voir', 'favoriser'): 1, ('favoriser', 'hand'): 1, ('hand', 'sport'): 1, ('sport', 'petit.'): 1, ('petit.', 'lew'): 1, ('lew', 'parents'): 1, ('parents', 'pouvait'): 1, ('pouvait', 'aid...'): 1, ('aid...', 'oui'): 1, ('oui', 'enlever'): 1, ('enlever', 'spasticit√©'): 1, ('spasticit√©', 'membres'): 1, ('membres', 'inf√©rieu...'): 1, ('inf√©rieu...',): 1}\n",
            "Probabilit√© de la target: 0.02\n",
            "Probabilit√© de chacun des mots: {'existait': 0.019230769230769232, 'truc': 0.019230769230769232, 'faire‚Ä¶': 0.019230769230769232, 'fauteuil': 0.019230769230769232, '√©lectrique': 0.019230769230769232, 'd√©tection': 0.019230769230769232, 'danger': 0.019230769230769232, 'militer': 0.019230769230769232, 'langage': 0.019230769230769232, 'signes': 0.019230769230769232, 'ad...': 0.019230769230769232, 'ouvrir': 0.019230769230769232, 'porte': 0.019230769230769232, 'placar': 0.019230769230769232, 'telecomm...': 0.019230769230769232, 'faut': 0.019230769230769232, 'd√©composer': 0.019230769230769232, 'geste': 0.019230769230769232, 'construit': 0.019230769230769232, 'd...': 0.019230769230769232, 'attacher': 0.019230769230769232, 'v√©hicule': 0.019230769230769232, '...': 0.019230769230769232, 'transforme': 0.019230769230769232, 'run': 0.019230769230769232, 'roulant': 0.019230769230769232, 'fauteu...': 0.019230769230769232, 'd√©ambulateur': 0.019230769230769232, 'performant': 0.019230769230769232, 'pliable': 0.019230769230769232, 'aider': 0.019230769230769232, 'alerter': 0.019230769230769232, 'automatiquement': 0.019230769230769232, 'fourni...': 0.019230769230769232, 'syst√®me': 0.019230769230769232, 'permettre': 0.019230769230769232, 'lecture': 0.019230769230769232, 'lit': 0.019230769230769232, '(o...': 0.019230769230769232, 'pouvoir': 0.019230769230769232, 'appeler': 0.019230769230769232, 't√©l√©phon...': 0.019230769230769232, \"s'il\": 0.019230769230769232, '<e>': 0.019230769230769232, '<oov>': 0.019230769230769232}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf55evefw_Uv"
      },
      "source": [
        "Dans cette section nous avons voulu explorer nos donn√©es. Nous chercher les mots qui apparaissaient le plus de fois puis leur fr√©quence exacte. Nous avons aussi introduit la fonctionnalit√© de N-grams qui permet de d√©compter tous les mots du dataset ainsi que de possibles couples de mots. Il est int√©ressant de voir que certains mots vont par pairs comme \"fauteuils roulants\" ou \"langue signes\". Ces couples peuvent avoir une fr√©quence √©l√©v√©e et peuvent nous r√©l√©ver des indices sur les besoins ou les pricipaux th√®mes des r√©ponses. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO-aDaNEE9zr"
      },
      "source": [
        "# Pour aller plus loin "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krHmuVFOCuoY"
      },
      "source": [
        "> Pour aller plus loin et ainsi une meilleure compr√©hension, voici une courte documentation sur les fonctionnalit√©s utilis√©es dans la fonction. \n",
        "\n",
        "> Rappel : toutes les fonctions utilis√©es dans cette section se trouvent dans les fichiers python (.py) appel√©s **Fr√©quence** et **Neighboor** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fku6laGQ9280"
      },
      "source": [
        "**N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_k52R7k-AMQ"
      },
      "source": [
        "\n",
        "\n",
        "Dans cette section, nous allons d√©velopper le mod√®le de langage n-grams.\n",
        "- Supposons que la probabilit√© du mot suivant ne d√©pende que du n-gramme pr√©c√©dent.\n",
        "- Le n-gramme pr√©c√©dent est la s√©rie des \"n\" mots pr√©c√©dents.\n",
        "\n",
        "La probabilit√© conditionnelle pour le mot √† la position \"t\" dans la phrase, √©tant donn√© que les mots qui le pr√©c√®dent sont $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ is:\n",
        "\n",
        "$$ P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}$$\n",
        "\n",
        "Nous pouvons estimer cette probabilit√© en comptant les occurrences de ces s√©ries de mots dans les donn√©es de formation.\n",
        "- La probabilit√© peut √™tre estim√©e sous la forme d'un rapport, o√π\n",
        "- Le num√©rateur est le nombre de fois que le mot \"t\" appara√Æt apr√®s les mots t-1 √† t-n dans les donn√©es de formation.\n",
        "- Le d√©nominateur est le nombre de fois que les mots t-1 √† t-n apparaissent dans les donn√©es d'entra√Ænement.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
        "\n",
        "- La fonction $C(\\cdots)$ indique le nombre d'occurrences de la s√©quence donn√©e. \n",
        "- La fonction $\\hat{P}$ d√©signe l'estimation de $P$. \n",
        "- Notez que le d√©nominateur de l'√©quation (2) est le nombre d'occurrences des mots $n$ pr√©c√©dents, et le num√©rateur est la m√™me s√©quence suivie du mot $w_t$.\n",
        "\n",
        "Plus tard, nous modifierons l'√©quation (2) en ajoutant un lissage k, qui √©vite les erreurs lorsque les comptes sont nuls.\n",
        "\n",
        "L'√©quation (2) nous dit que pour estimer les probabilit√©s bas√©es sur les n-grammes, nous avons besoin des nombres de n-grammes (pour le d√©nominateur) et de (n+1)-grammes (pour le num√©rateur).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jLltyhW8zMy"
      },
      "source": [
        "**Counts-grams**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIlMXUyM856-"
      },
      "source": [
        "Nous cr√©ons une fonction qui calcule les comptes de n-grammes pour un nombre arbitraire ùëõ. \n",
        "Lors du calcul du nombre de n-grammes, pr√©parer la phrase √† l'avance en pr√©parant ùëõ-1\n",
        "\n",
        "des marqueurs de d√©part \"< s >\" pour indiquer le d√©but de la phrase.\n",
        "\n",
        "    - Par exemple, dans le mod√®le du bi-gramme (N=2), une s√©quence avec deux marqueurs de d√©part \"<s><s>\" devrait pr√©dire le premier mot d'une phrase.\n",
        "    - Ainsi, si la phrase est \"J'aime la nourriture\", modifiez la pour qu'elle soit \"<s><s> J'aime la nourriture\".\n",
        "    - Pr√©parez √©galement la phrase pour le comptage en ajoutant un jeton de fin \"<e>\" afin que le mod√®le puisse pr√©dire quand terminer une phrase.\n",
        "\n",
        "Note technique : dans cette impl√©mentation, vous stockerez les comptages sous forme de dictionnaire.\n",
        "\n",
        "    - La cl√© de chaque paire cl√©-valeur dans le dictionnaire est un tuple de n mots (et non une liste)\n",
        "    - La valeur dans la paire cl√©-valeur est le nombre d'occurrences.\n",
        "    - La raison pour laquelle on utilise un tuple comme cl√© au lieu d'une liste est qu'une liste en Python est un objet mutable (elle peut √™tre modifi√©e apr√®s sa cr√©ation). Un tuple est \"immuable\", c'est-√†-dire qu'il ne peut pas √™tre modifi√© apr√®s sa cr√©ation. Un tuple peut donc √™tre utilis√© comme type de donn√©es pour la cl√© d'un dictionnaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAiE2KOj9fti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aca9098-14df-4bf2-ce4f-3e5b3060bad1"
      },
      "source": [
        "print(\"Uni-gram:\")\n",
        "print(count_n_grams(d3_net, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_n_grams(d3_net, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uni-gram:\n",
            "{('existait',): 2, ('truc',): 4, ('faire‚Ä¶',): 1, ('fauteuil',): 3, ('√©lectrique',): 1, ('d√©tection',): 1, ('danger',): 1, ('militer',): 1, ('langage',): 1, ('signes',): 1, ('ad...',): 1, ('ouvrir',): 1, ('porte',): 1, ('placar',): 1, ('telecomm...',): 1, ('faut',): 1, ('d√©composer',): 1, ('geste',): 1, ('construit',): 1, ('d...',): 1, ('attacher',): 3, ('v√©hicule',): 1, ('...',): 4, ('transforme',): 1, ('run',): 1, ('roulant',): 1, ('fauteu...',): 2, ('d√©ambulateur',): 1, ('performant',): 1, ('pliable',): 1, ('aider',): 2, ('alerter',): 1, ('automatiquement',): 1, ('fourni...',): 1, ('syst√®me',): 1, ('permettre',): 1, ('lecture',): 1, ('lit',): 1, ('(o...',): 1, ('pouvoir',): 1, ('appeler',): 1, ('t√©l√©phon...',): 1, (\"s'il\",): 1, ('contorsioner',): 1, ('parcs',): 1, ('jeux',): 1, ('adapt√©s',): 1, ('!!!',): 1, ('plages',): 1, ('domotique',): 1, ('fermer',): 1, ('domicile?',): 1, ('solutions',): 1, ('correctement',): 1, ('mo...',): 1, ('www.changing-places.org',): 1, ('voir',): 1, ('favoriser',): 1, ('hand',): 1, ('sport',): 1, ('petit.',): 1, ('lew',): 1, ('parents',): 1, ('pouvait',): 1, ('aid...',): 1, ('oui',): 1, ('enlever',): 1, ('spasticit√©',): 1, ('membres',): 1, ('inf√©rieu...',): 1}\n",
            "Bi-gram:\n",
            "{('existait', 'truc'): 2, ('truc', 'faire‚Ä¶'): 1, ('faire‚Ä¶', 'fauteuil'): 1, ('fauteuil', '√©lectrique'): 1, ('√©lectrique', 'd√©tection'): 1, ('d√©tection', 'danger'): 1, ('danger', 'militer'): 1, ('militer', 'langage'): 1, ('langage', 'signes'): 1, ('signes', 'ad...'): 1, ('ad...', 'ouvrir'): 1, ('ouvrir', 'porte'): 1, ('porte', 'placar'): 1, ('placar', 'telecomm...'): 1, ('telecomm...', 'faut'): 1, ('faut', 'd√©composer'): 1, ('d√©composer', 'geste'): 1, ('geste', 'construit'): 1, ('construit', 'd...'): 1, ('d...', 'attacher'): 1, ('attacher', 'fauteuil'): 1, ('fauteuil', 'v√©hicule'): 1, ('v√©hicule', '...'): 1, ('...', 'transforme'): 1, ('transforme', 'run'): 1, ('run', 'fauteuil'): 1, ('fauteuil', 'roulant'): 1, ('roulant', 'fauteu...'): 1, ('fauteu...', 'd√©ambulateur'): 1, ('d√©ambulateur', 'performant'): 1, ('performant', 'pliable'): 1, ('pliable', 'aider'): 1, ('aider', '...'): 2, ('...', 'truc'): 1, ('truc', 'alerter'): 1, ('alerter', 'automatiquement'): 1, ('automatiquement', 'fourni...'): 1, ('fourni...', 'syst√®me'): 1, ('syst√®me', 'permettre'): 1, ('permettre', 'lecture'): 1, ('lecture', 'lit'): 1, ('lit', '(o...'): 1, ('(o...', 'truc'): 1, ('truc', 'pouvoir'): 1, ('pouvoir', 'appeler'): 1, ('appeler', 't√©l√©phon...'): 1, ('t√©l√©phon...', \"s'il\"): 1, (\"s'il\", 'existait'): 1, ('truc', 'aider'): 1, ('...', 'contorsioner'): 1, ('contorsioner', 'attacher'): 1, ('attacher', 'fauteu...'): 1, ('fauteu...', 'parcs'): 1, ('parcs', 'jeux'): 1, ('jeux', 'adapt√©s'): 1, ('adapt√©s', '!!!'): 1, ('!!!', 'plages'): 1, ('plages', '...'): 1, ('...', 'domotique'): 1, ('domotique', 'fermer'): 1, ('fermer', 'domicile?'): 1, ('domicile?', 'solutions'): 1, ('solutions', 'attacher'): 1, ('attacher', 'correctement'): 1, ('correctement', 'mo...'): 1, ('mo...', 'www.changing-places.org'): 1, ('www.changing-places.org', 'voir'): 1, ('voir', 'favoriser'): 1, ('favoriser', 'hand'): 1, ('hand', 'sport'): 1, ('sport', 'petit.'): 1, ('petit.', 'lew'): 1, ('lew', 'parents'): 1, ('parents', 'pouvait'): 1, ('pouvait', 'aid...'): 1, ('aid...', 'oui'): 1, ('oui', 'enlever'): 1, ('enlever', 'spasticit√©'): 1, ('spasticit√©', 'membres'): 1, ('membres', 'inf√©rieu...'): 1, ('inf√©rieu...',): 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysU_gns-9ryQ"
      },
      "source": [
        "**Estimer la probabilit√© d'un mot target**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUoEYpZ994jY"
      },
      "source": [
        "Nous voulons estimer la probabilit√© d'un mot donn√© par rapport aux \"n\" mots pr√©c√©dents en utilisant le nombre de n-grammes.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
        "\n",
        "Cette formule ne fonctionne pas quand le compte d'un n-gramme est √©gal √† z√©ro..\n",
        "- Supposons que nous rencontrions un n-gram qui ne figurait pas dans les donn√©es de formation.  \n",
        "- Alors, l'√©quation (2) ne peut pas √™tre √©valu√©e (elle devient z√©ro divis√© par z√©ro).\n",
        "\n",
        "Une fa√ßon de traiter les comptes de z√©ros est d'ajouter un lissage k.  \n",
        "- Le K-smoothing ajoute une constante positive $k$ √† chaque num√©rateur et $k \\times |V|$ au d√©nominateur, o√π $|V|$ est le nombre de mots du vocabulaire.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
        "\n",
        "\n",
        "Pour les n-grammes qui ont un compte de z√©ro, l'√©quation (3) devient $\\frac{1}{|V|}$.\n",
        "- Cela signifie que tout n-gramme ayant une valeur nulle a la m√™me probabilit√© de $\\frac{1}{|V|}$.\n",
        "\n",
        "D√©finissez une fonction qui calcule l'estimation de la probabilit√© (3) √† partir du nombre de n-grammes et d'une constante $k$.\n",
        "\n",
        "- La fonction prend dans un dictionnaire \"n_gram_counts\", o√π la cl√© est le n-gram et la valeur est le nombre de ce n-gram.\n",
        "- La fonction prend √©galement un autre dictionnaire \"n_plus1_gram_counts\", que vous utiliserez pour trouver le compte du n-gram pr√©c√©dent plus le mot courant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT9NmK7u99F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d911df8e-4d05-4a90-da33-b5aa42303a58"
      },
      "source": [
        "#test\n",
        "unique_words = [d3_net[i] for i in range(0, 50)]\n",
        "\n",
        "unigram_counts = count_n_grams(d3_net, 1)\n",
        "bigram_counts = count_n_grams(d3_net, 2)\n",
        "tmp_prob = estimate_probability(d3_net[3], d3_net[2], unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\"La probabilit√© estim√©e du mot 'faire', compte tenu du n-gramme 'truc' pr√©c√©dent, est: {tmp_prob:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La probabilit√© estim√©e du mot 'faire', compte tenu du n-gramme 'truc' pr√©c√©dent, est: 0.0200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6d0IFyG8Xq8"
      },
      "source": [
        "**Estimer la probabilit√© de tous les mots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UByjlXyY8gZS"
      },
      "source": [
        "La fonction d√©finie ci-dessous fait une boucle sur tous les mots du vocabulaire pour calculer les probabilit√©s de tous les mots possibles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LedbZmHB8iAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632c86c7-b3b1-44e4-a9bb-264ea5a5174b"
      },
      "source": [
        "\n",
        "#test\n",
        "unique_words = [d3_net[i] for i in range(0, 50)]\n",
        "unigram_counts = count_n_grams(d3_net, 1)\n",
        "bigram_counts = count_n_grams(d3_net, 2)\n",
        "prob_globale = estimate_probabilities(d3_net[2], unigram_counts, bigram_counts, unique_words, k=1)\n",
        "print(prob_globale)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'existait': 0.019230769230769232, 'truc': 0.019230769230769232, 'faire‚Ä¶': 0.019230769230769232, 'fauteuil': 0.019230769230769232, '√©lectrique': 0.019230769230769232, 'd√©tection': 0.019230769230769232, 'danger': 0.019230769230769232, 'militer': 0.019230769230769232, 'langage': 0.019230769230769232, 'signes': 0.019230769230769232, 'ad...': 0.019230769230769232, 'ouvrir': 0.019230769230769232, 'porte': 0.019230769230769232, 'placar': 0.019230769230769232, 'telecomm...': 0.019230769230769232, 'faut': 0.019230769230769232, 'd√©composer': 0.019230769230769232, 'geste': 0.019230769230769232, 'construit': 0.019230769230769232, 'd...': 0.019230769230769232, 'attacher': 0.019230769230769232, 'v√©hicule': 0.019230769230769232, '...': 0.019230769230769232, 'transforme': 0.019230769230769232, 'run': 0.019230769230769232, 'roulant': 0.019230769230769232, 'fauteu...': 0.019230769230769232, 'd√©ambulateur': 0.019230769230769232, 'performant': 0.019230769230769232, 'pliable': 0.019230769230769232, 'aider': 0.019230769230769232, 'alerter': 0.019230769230769232, 'automatiquement': 0.019230769230769232, 'fourni...': 0.019230769230769232, 'syst√®me': 0.019230769230769232, 'permettre': 0.019230769230769232, 'lecture': 0.019230769230769232, 'lit': 0.019230769230769232, '(o...': 0.019230769230769232, 'pouvoir': 0.019230769230769232, 'appeler': 0.019230769230769232, 't√©l√©phon...': 0.019230769230769232, \"s'il\": 0.019230769230769232, '<e>': 0.019230769230769232, '<oov>': 0.019230769230769232}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oPw7DNs-XkI"
      },
      "source": [
        "**Matrices de probabilit√©s**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5KviM5i-vmM"
      },
      "source": [
        "Ce tableau ci-dessous pr√©sente la pr√©mi√®re matrice de probabilit√©s que nous r√©alisons ici. Elle nous premet de voir quels peuvent √™tre les mots corr√©l√©s dans notre texte. Elle nous montre aussi deux types de corr√©lation. Le premier est le type de mot qui doivent √™tre associ√©s √† un autre pour avoir du sens, comme \"langage signes\". Le second type est une corr√©lation qui donnerait au mot plus d'impact ou des associations de mots qui transmettraient une id√©e comme \"militer langage\", \"pouvoir appeler\" ou encore \"d√©tection danger\".\n",
        "\n",
        "Ce qui pourrait maintenant √™tre fait est de s√©parer ces deux types et garder la seconde mais transformer la premi√®re. Il faudrait cr√©er une liste d'expression qui pourrait revenir comme \"fauteuil roulant\" ou \"langage signes\", qui pourraient √™tre remplacer par un mot √©quivalent. Cela apporterait un nettoyage dans cette matrices et nous permettrait de sans doute observer des th√®mes ou extraire des id√©es du textes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENiF7EuR-eTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "f75dd419-396a-4899-b4a2-1760be4ed8c8"
      },
      "source": [
        "unique_words = [d3_net[i] for i in range(0, 70)]\n",
        "bigram_counts = count_n_grams(d3_net, 2)\n",
        "\n",
        "print('bigram counts')\n",
        "display(make_count_matrix(bigram_counts, unique_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bigram counts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>faire‚Ä¶</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>√©lectrique</th>\n",
              "      <th>d√©tection</th>\n",
              "      <th>danger</th>\n",
              "      <th>militer</th>\n",
              "      <th>langage</th>\n",
              "      <th>signes</th>\n",
              "      <th>ad...</th>\n",
              "      <th>ouvrir</th>\n",
              "      <th>porte</th>\n",
              "      <th>placar</th>\n",
              "      <th>telecomm...</th>\n",
              "      <th>faut</th>\n",
              "      <th>d√©composer</th>\n",
              "      <th>geste</th>\n",
              "      <th>construit</th>\n",
              "      <th>d...</th>\n",
              "      <th>attacher</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>v√©hicule</th>\n",
              "      <th>...</th>\n",
              "      <th>transforme</th>\n",
              "      <th>run</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>roulant</th>\n",
              "      <th>fauteu...</th>\n",
              "      <th>d√©ambulateur</th>\n",
              "      <th>performant</th>\n",
              "      <th>pliable</th>\n",
              "      <th>aider</th>\n",
              "      <th>...</th>\n",
              "      <th>truc</th>\n",
              "      <th>alerter</th>\n",
              "      <th>automatiquement</th>\n",
              "      <th>fourni...</th>\n",
              "      <th>syst√®me</th>\n",
              "      <th>permettre</th>\n",
              "      <th>lecture</th>\n",
              "      <th>lit</th>\n",
              "      <th>(o...</th>\n",
              "      <th>truc</th>\n",
              "      <th>pouvoir</th>\n",
              "      <th>appeler</th>\n",
              "      <th>t√©l√©phon...</th>\n",
              "      <th>s'il</th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>aider</th>\n",
              "      <th>...</th>\n",
              "      <th>contorsioner</th>\n",
              "      <th>attacher</th>\n",
              "      <th>fauteu...</th>\n",
              "      <th>parcs</th>\n",
              "      <th>jeux</th>\n",
              "      <th>adapt√©s</th>\n",
              "      <th>!!!</th>\n",
              "      <th>plages</th>\n",
              "      <th>...</th>\n",
              "      <th>domotique</th>\n",
              "      <th>fermer</th>\n",
              "      <th>domicile?</th>\n",
              "      <th>solutions</th>\n",
              "      <th>attacher</th>\n",
              "      <th>correctement</th>\n",
              "      <th>mo...</th>\n",
              "      <th>www.changing-places.org</th>\n",
              "      <th>voir</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;oov&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(aider,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(syst√®me,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(construit,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(domotique,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(correctement,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(d√©ambulateur,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>()</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(faut,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(attacher,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(t√©l√©phon...,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows √ó 72 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 existait  truc  faire‚Ä¶  ...  voir  <e>  <oov>\n",
              "(aider,)              0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(syst√®me,)            0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(construit,)          0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(domotique,)          0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(correctement,)       0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "...                   ...   ...     ...  ...   ...  ...    ...\n",
              "(d√©ambulateur,)       0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "()                    0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(faut,)               0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(attacher,)           0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "(t√©l√©phon...,)        0.0   0.0     0.0  ...   0.0  0.0    0.0\n",
              "\n",
              "[70 rows x 72 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nrw3OPpMMaF"
      },
      "source": [
        "Les matrices suivantes sont d'autres matrices de probabilit√©s. Elles montrent la probabilit√© des mots du texte en fonction des autres mots. La premi√®re est une matrice de probabilit√© effectuant la fonctionalit√© bi-gram soit par couple de mots. Elle nous montre la probabilit√© d'un mot par rapport √† un autre. Par exmple, si il existe le mot \"v√©hicule\" dans le texte, en fonction de ce texte, la probabilit√© que il y ait aussi \"existait\" est de 0.018868. \n",
        "\n",
        "La seconde matrice utilise les fonctionalit√©s du trigram. Cela signifie que par couple de mots, il matrice calculera la probabilit√© d'un mot. Par exemple ici, la matrice choisit un couple de mots du texte (ouvrir, porte). Ces mots semblent tr√®s corr√©l√©s. Elle montre ensuite une probabilit√© importante de l'existance du mot \"placar\" soit 0.0377. Cela peut √™tre interpr√©ter par le fait que la matrice trouve des mots corr√©l√©s qui peuvent faire partis d'un th√®me particulier, comme ici le fait d'ouvrir une porte. Puis elle pr√©duit qu'il ait une importante probabilit√© que d'autres mots similaires ou appartenant au m√™me th√®me apparaissent. Le mot \"placar\" peut √™tre simialire ici, au mot \"porte\" car il n√©cessite la m√™me action pour l'utiliser soit le fait d'ouvrir. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ1VGEvDKV1s"
      },
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, vocabulary)\n",
        "    count_matrix += k\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KnO18RB-6F0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "d5115a9f-eb17-4c09-a2b3-cb8da8fad1eb"
      },
      "source": [
        "unique_words = [d3_net[i] for i in range(0, 50)]\n",
        "bigram_counts = count_n_grams(d3_net, 2)\n",
        "print(\"bigram probabilities\")\n",
        "#print(make_probability_matrix(bigram_counts, unique_words, k=1))\n",
        "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bigram probabilities\n",
            "2021-01-06 21:26:30,460 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>faire‚Ä¶</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>√©lectrique</th>\n",
              "      <th>d√©tection</th>\n",
              "      <th>danger</th>\n",
              "      <th>militer</th>\n",
              "      <th>langage</th>\n",
              "      <th>signes</th>\n",
              "      <th>ad...</th>\n",
              "      <th>ouvrir</th>\n",
              "      <th>porte</th>\n",
              "      <th>placar</th>\n",
              "      <th>telecomm...</th>\n",
              "      <th>faut</th>\n",
              "      <th>d√©composer</th>\n",
              "      <th>geste</th>\n",
              "      <th>construit</th>\n",
              "      <th>d...</th>\n",
              "      <th>attacher</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>v√©hicule</th>\n",
              "      <th>...</th>\n",
              "      <th>transforme</th>\n",
              "      <th>run</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>roulant</th>\n",
              "      <th>fauteu...</th>\n",
              "      <th>d√©ambulateur</th>\n",
              "      <th>performant</th>\n",
              "      <th>pliable</th>\n",
              "      <th>aider</th>\n",
              "      <th>...</th>\n",
              "      <th>truc</th>\n",
              "      <th>alerter</th>\n",
              "      <th>automatiquement</th>\n",
              "      <th>fourni...</th>\n",
              "      <th>syst√®me</th>\n",
              "      <th>permettre</th>\n",
              "      <th>lecture</th>\n",
              "      <th>lit</th>\n",
              "      <th>(o...</th>\n",
              "      <th>truc</th>\n",
              "      <th>pouvoir</th>\n",
              "      <th>appeler</th>\n",
              "      <th>t√©l√©phon...</th>\n",
              "      <th>s'il</th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;oov&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(aider,)</th>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(syst√®me,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(construit,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(domotique,)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(correctement,)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(d√©ambulateur,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>()</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(faut,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(attacher,)</th>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.018519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(t√©l√©phon...,)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows √ó 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 existait      truc    faire‚Ä¶  ...      truc       <e>     <oov>\n",
              "(aider,)         0.018519  0.018519  0.018519  ...  0.018519  0.018519  0.018519\n",
              "(syst√®me,)       0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "(construit,)     0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "(domotique,)     0.019231  0.019231  0.019231  ...  0.019231  0.019231  0.019231\n",
              "(correctement,)  0.019231  0.019231  0.019231  ...  0.019231  0.019231  0.019231\n",
              "...                   ...       ...       ...  ...       ...       ...       ...\n",
              "(d√©ambulateur,)  0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "()               0.019231  0.019231  0.019231  ...  0.019231  0.019231  0.019231\n",
              "(faut,)          0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "(attacher,)      0.018519  0.018519  0.018519  ...  0.018519  0.018519  0.018519\n",
              "(t√©l√©phon...,)   0.018868  0.018868  0.018868  ...  0.018868  0.018868  0.018868\n",
              "\n",
              "[70 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZyBw-NY_AbV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "c7ea0fb8-61dd-42cb-f700-e59b827a3ba3"
      },
      "source": [
        "print(\"trigram probabilities\")\n",
        "trigram_counts = count_n_grams(d3_net, 3)\n",
        "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trigram probabilities\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>faire‚Ä¶</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>√©lectrique</th>\n",
              "      <th>d√©tection</th>\n",
              "      <th>danger</th>\n",
              "      <th>militer</th>\n",
              "      <th>langage</th>\n",
              "      <th>signes</th>\n",
              "      <th>ad...</th>\n",
              "      <th>ouvrir</th>\n",
              "      <th>porte</th>\n",
              "      <th>placar</th>\n",
              "      <th>telecomm...</th>\n",
              "      <th>faut</th>\n",
              "      <th>d√©composer</th>\n",
              "      <th>geste</th>\n",
              "      <th>construit</th>\n",
              "      <th>d...</th>\n",
              "      <th>attacher</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>v√©hicule</th>\n",
              "      <th>...</th>\n",
              "      <th>transforme</th>\n",
              "      <th>run</th>\n",
              "      <th>fauteuil</th>\n",
              "      <th>roulant</th>\n",
              "      <th>fauteu...</th>\n",
              "      <th>d√©ambulateur</th>\n",
              "      <th>performant</th>\n",
              "      <th>pliable</th>\n",
              "      <th>aider</th>\n",
              "      <th>...</th>\n",
              "      <th>truc</th>\n",
              "      <th>alerter</th>\n",
              "      <th>automatiquement</th>\n",
              "      <th>fourni...</th>\n",
              "      <th>syst√®me</th>\n",
              "      <th>permettre</th>\n",
              "      <th>lecture</th>\n",
              "      <th>lit</th>\n",
              "      <th>(o...</th>\n",
              "      <th>truc</th>\n",
              "      <th>pouvoir</th>\n",
              "      <th>appeler</th>\n",
              "      <th>t√©l√©phon...</th>\n",
              "      <th>s'il</th>\n",
              "      <th>existait</th>\n",
              "      <th>truc</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;oov&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(jeux, adapt√©s)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(t√©l√©phon..., s'il)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(..., domotique)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(faut, d√©composer)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(lecture, lit)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(ouvrir, porte)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(fauteu..., parcs)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(enlever, spasticit√©)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(plages, ...)</th>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "      <td>0.019231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(telecomm..., faut)</th>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "      <td>0.018868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81 rows √ó 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       existait      truc  ...       <e>     <oov>\n",
              "(jeux, adapt√©s)        0.019231  0.019231  ...  0.019231  0.019231\n",
              "(t√©l√©phon..., s'il)    0.018868  0.018868  ...  0.018868  0.018868\n",
              "(..., domotique)       0.019231  0.019231  ...  0.019231  0.019231\n",
              "(faut, d√©composer)     0.018868  0.018868  ...  0.018868  0.018868\n",
              "(lecture, lit)         0.018868  0.018868  ...  0.018868  0.018868\n",
              "...                         ...       ...  ...       ...       ...\n",
              "(ouvrir, porte)        0.018868  0.018868  ...  0.018868  0.018868\n",
              "(fauteu..., parcs)     0.019231  0.019231  ...  0.019231  0.019231\n",
              "(enlever, spasticit√©)  0.019231  0.019231  ...  0.019231  0.019231\n",
              "(plages, ...)          0.019231  0.019231  ...  0.019231  0.019231\n",
              "(telecomm..., faut)    0.018868  0.018868  ...  0.018868  0.018868\n",
              "\n",
              "[81 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1BvjTRGn-K8"
      },
      "source": [
        "La prochaine √©tape serait de garder dans un nouveau tableau les mots reli√©s √† une probabilit√© sup√©rieur √† 0.030. Certains besoins ou th√®mes vont pouvoir apparaitre. Cela aidera pour la prochaine √©tape de clustering puis de l'√©laboration de l'algorithm de machine learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_D60dslzfhQ"
      },
      "source": [
        "# Lemmatization et racine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uK2ynCXlys"
      },
      "source": [
        "Voic un des r√©sultats de lemmatization. Cette fonction ci-dessous nous donne une lemmatization tr√®s simplifi√©e. Elle n'est pas la plus efficace. Nous conseillons l'utilisation des librairies **spaCy**. Ici, nous ne utilisons pas mais potentiellement elles serait plus int√©ressante car elle est travaill√©e pour diff√©rentes langues . Elle ne s'applique pas uniquement pour l'anglais mais aussi pour des textes fran√ßais ou encore allemand.\n",
        "\n",
        "Cependant, les r√©sultats que nous obtenons sont satisfaisants.Une grande partie des mots sont ramen√©s √† leur racine. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0EH1ndSK2uV"
      },
      "source": [
        "def lem(data): \n",
        "\n",
        "  # stemming \n",
        "  porter_stemmer = nlp.PorterStemmer()\n",
        "  roots = [porter_stemmer.stem(each) for each in d3_net]\n",
        "  print(\"result of stemming: \",roots)\n",
        "\n",
        "  # lemmatization \n",
        "  lemma = nlp.WordNetLemmatizer()\n",
        "  lem_roots = [lemma.lemmatize(each) for each in roots]\n",
        "  print(\"result of lemmatization: \",lem_roots)\n",
        "\n",
        "  return lem_roots \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LogxAjOiLShF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a3baa4-66bc-4bf4-c252-1c2889582666"
      },
      "source": [
        "'''d1_lem = lem(d1_net)\n",
        "d2_lem = lem(d2_net)\n",
        "d3_lem = lem(d3_net)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'d1_lem = lem(d1_net)\\nd2_lem = lem(d2_net)\\nd3_lem = lem(d3_net)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rROBNuykOxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034c5cba-6d11-4cd8-ceac-11b0d779a4dc"
      },
      "source": [
        "d3_lem = lem(d3_net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result of stemming:  ['existait', 'truc', 'faire‚Ä¶', 'fauteuil', '√©lectriqu', 'd√©tection', 'danger', 'milit', 'langag', 'sign', 'ad...', 'ouvrir', 'port', 'placar', 'telecomm...', 'faut', 'd√©compos', 'gest', 'construit', 'd...', 'attach', 'fauteuil', 'v√©hicul', '...', 'transform', 'run', 'fauteuil', 'roulant', 'fauteu...', 'd√©ambulateur', 'perform', 'pliabl', 'aider', '...', 'truc', 'alert', 'automatiqu', 'fourni...', 'syst√®m', 'permettr', 'lectur', 'lit', '(o...', 'truc', 'pouvoir', 'appel', 't√©l√©phon...', \"s'il\", 'existait', 'truc', 'aider', '...', 'contorsion', 'attach', 'fauteu...', 'parc', 'jeux', 'adapt√©', '!!!', 'plage', '...', 'domotiqu', 'fermer', 'domicile?', 'solut', 'attach', 'correct', 'mo...', 'www.changing-places.org', 'voir', 'favoris', 'hand', 'sport', 'petit.', 'lew', 'parent', 'pouvait', 'aid...', 'oui', 'enlev', 'spasticit√©', 'membr', 'inf√©rieu...']\n",
            "result of lemmatization:  ['existait', 'truc', 'faire‚Ä¶', 'fauteuil', '√©lectriqu', 'd√©tection', 'danger', 'milit', 'langag', 'sign', 'ad...', 'ouvrir', 'port', 'placar', 'telecomm...', 'faut', 'd√©compos', 'gest', 'construit', 'd...', 'attach', 'fauteuil', 'v√©hicul', '...', 'transform', 'run', 'fauteuil', 'roulant', 'fauteu...', 'd√©ambulateur', 'perform', 'pliabl', 'aider', '...', 'truc', 'alert', 'automatiqu', 'fourni...', 'syst√®m', 'permettr', 'lectur', 'lit', '(o...', 'truc', 'pouvoir', 'appel', 't√©l√©phon...', \"s'il\", 'existait', 'truc', 'aider', '...', 'contorsion', 'attach', 'fauteu...', 'parc', 'jeux', 'adapt√©', '!!!', 'plage', '...', 'domotiqu', 'fermer', 'domicile?', 'solut', 'attach', 'correct', 'mo...', 'www.changing-places.org', 'voir', 'favoris', 'hand', 'sport', 'petit.', 'lew', 'parent', 'pouvait', 'aid...', 'oui', 'enlev', 'spasticit√©', 'membr', 'inf√©rieu...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfI6Twc9MrNr"
      },
      "source": [
        "# Clustering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjvECIr8zskI"
      },
      "source": [
        "## Trouver des r√©ponses similaires "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz955hAYVKrv"
      },
      "source": [
        "### Premi√®re m√©thode : Nearest Neighbor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBBy0rmSl2P9"
      },
      "source": [
        "Dans cette section, le but est trouv√© des r√©ponses similaires et de les rassembler ensembles. Nous allons introduire un document comprenant les \"embeddings\", soit le chiffrement des mots. Nous allons les utilis√©s pour assimil√©s les mots de notre dataset avec un chiffrement qui sera lui-m√™me utilis√© pour vectoriser ces mots. \n",
        "\n",
        "Ces vecteurs vont √™tre compar√©s et ainsi, analys√©s pour ainsi les rassembler en groupe. Gr√¢ce √† cela, nous pourrons comment √† classifier les types de r√©ponses et nettoyer encore plus profondemment notre dataset. \n",
        "\n",
        "Cette √©tape est notre premier √©tape de clustering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44HuXWApnFhI"
      },
      "source": [
        "Nous importons un document qui contient les √©quivalents chiffr√©s de chaque mot de notre dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fxLtV58k350"
      },
      "source": [
        "fr_embeddings = os.path.join('/content/fr_embeddings.p')\n",
        "fr_embeddings = str(fr_embeddings)\n",
        "d3_lem = str(d3_lem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r12RjiVvnZgd"
      },
      "source": [
        "Dans un premier temps, nous allons associer aux mots pr√©sents dans notre texte des embeddings, puis nous allons vectoriser ces mots. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNibX1i6n-FE"
      },
      "source": [
        "def mot_vecteur(data, fr_embeddings): \n",
        "\n",
        "  # associer chiffrement aux mots \n",
        "  mot_embedding = get_document_embedding(data, fr_embeddings)\n",
        "  #print('Mots chiffr√©s:', mot_embedding)\n",
        "\n",
        "  # Vectoriser les mots \n",
        "  mot_vect, ind2data = get_document_vecs(data, fr_embeddings)\n",
        "  #print('Mots vectoris√©s:', mot_vect, ind2data)\n",
        "\n",
        "  return mot_embedding, mot_vect, ind2data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnANHZRjo8oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c53a91-bc93-4f37-fafb-07ccbde1dfa5"
      },
      "source": [
        "mot_embedding, mot_vect, ind2data = mot_vecteur(d3_lem, fr_embeddings)\n",
        "print(mot_vect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy0745_GkFZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c719df-8ae1-4661-a8eb-6a4894599250"
      },
      "source": [
        "print(f\"length of dictionary {len(ind2data)}\")\n",
        "print(f\"shape of document_vecs {mot_vect.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of dictionary 859\n",
            "shape of document_vecs (859, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DLkiHMFx4lz"
      },
      "source": [
        "Maintenant que nous avons cr√©er ces vecteurs, nous voulons assembler entre elles les r√©ponses les plus similaires. \n",
        "\n",
        "Pour cela, nous allons cr√©er des tables de hachages et appeler les fonctions de clustering comme K-means et K-neighboors. Ces fonctions vont permettre de rassembler les r√©ponses les plus simalaires gr√¢ce √† leur vecteurs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-2guHzpkJtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d118c7e6-abfb-4c28-90b5-0dea7aefebc4"
      },
      "source": [
        "# this gives you a similar tweet as your input.\n",
        "# this implementation is vectorized...\n",
        "import numpy as np\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "idx = np.argmax(cosine_similarity(mot_vect, mot_embedding))\n",
        "print(d3_lem[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTe7TsgikMU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5060171-dae1-4506-f630-28e0dfa59b60"
      },
      "source": [
        "N_VECS = len(d3_lem)       # This many vectors.\n",
        "N_DIMS = len(ind2data[1])     # Vector dimensionality.\n",
        "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of vectors is 859 and each has 300 dimensions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdrHqpcOkaHH"
      },
      "source": [
        "# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n",
        "N_PLANES = 10\n",
        "# Number of times to repeat the hashing to improve the search.\n",
        "N_UNIVERSES = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAF8TpzOkb28"
      },
      "source": [
        "np.random.seed(0)\n",
        "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
        "            for _ in range(N_UNIVERSES)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaJrVJFA0PAA"
      },
      "source": [
        "Nous appliquons donc les fonctions suivantes :\n",
        "\n",
        "* la table de hachage \n",
        "* l'approximation knn \n",
        "* le vecteur le plus proche "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1n8LPDXf1KW"
      },
      "source": [
        "np.random.seed(0)\n",
        "idx = 0\n",
        "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
        "vec = np.random.rand(1, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gTX34K01IL0"
      },
      "source": [
        "def table_hash(vec,document_vecs, planes) : \n",
        "\n",
        "  print(f\" The hash value for this vector,\",\n",
        "      f\"and the set of planes at index {idx},\",\n",
        "      f\"is {hash_value_of_vector(vec, planes)}\")\n",
        "  \n",
        "  tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
        "  print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
        "  print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
        "  print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")\n",
        "\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LGEGCMo2J4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d479cfa-db04-4852-ad6a-50f637b49841"
      },
      "source": [
        "hash = table_hash(vec, mot_vect, planes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The hash value for this vector, and the set of planes at index 0, is 768\n",
            "The hash table at key 0 has 859 document vectors\n",
            "The id table at key 0 has 859\n",
            "The first 5 document indices stored at key 0 of are [0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKJlFiUnlgzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ae3ab9-03ca-4f70-b351-cd5265b04fb0"
      },
      "source": [
        "# Creating the hashtables\n",
        "hash_tables = []\n",
        "id_tables = []\n",
        "for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n",
        "    print('working on hash universe #:', universe_id)\n",
        "    planes = planes_l[universe_id]\n",
        "    hash_table, id_table = make_hash_table(vec, planes)\n",
        "    hash_tables.append(hash_table)\n",
        "    id_tables.append(id_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "working on hash universe #: 0\n",
            "working on hash universe #: 1\n",
            "working on hash universe #: 2\n",
            "working on hash universe #: 3\n",
            "working on hash universe #: 4\n",
            "working on hash universe #: 5\n",
            "working on hash universe #: 6\n",
            "working on hash universe #: 7\n",
            "working on hash universe #: 8\n",
            "working on hash universe #: 9\n",
            "working on hash universe #: 10\n",
            "working on hash universe #: 11\n",
            "working on hash universe #: 12\n",
            "working on hash universe #: 13\n",
            "working on hash universe #: 14\n",
            "working on hash universe #: 15\n",
            "working on hash universe #: 16\n",
            "working on hash universe #: 17\n",
            "working on hash universe #: 18\n",
            "working on hash universe #: 19\n",
            "working on hash universe #: 20\n",
            "working on hash universe #: 21\n",
            "working on hash universe #: 22\n",
            "working on hash universe #: 23\n",
            "working on hash universe #: 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3xE_cGRfnRv"
      },
      "source": [
        "#document_vecs, ind2Tweet\n",
        "doc_id = 0\n",
        "doc_to_search = d3_lem[doc_id]\n",
        "#vec_to_search = mot_vect[doc_id]\n",
        "vec_to_search = ind2data[doc_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnHMTpwfif_e"
      },
      "source": [
        "Nous cherchons maintenant les voisins les plus proches de chaque vecteurs. Ces vecteurs sont chacune des r√©ponses des intervenants. Nous souhaitons trouver des r√©ponses similaires pour ainsi les rassembler et les traiter chacune diff√©remment. Cela nous permettra d'avoir des clusters et de les analyze chacun afin d'en extraire des besoins. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT4fWhY_lk4q"
      },
      "source": [
        "# This is the code used to do the fast nearest neighbor search. Feel free to go over it\n",
        "def approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
        "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
        "    assert num_universes_to_use <= N_UNIVERSES\n",
        "\n",
        "    # Vectors that will be checked as possible nearest neighbor\n",
        "    vecs_to_consider_l = list()\n",
        "\n",
        "    # list of document IDs\n",
        "    ids_to_consider_l = list()\n",
        "\n",
        "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
        "    ids_to_consider_set = set()\n",
        "\n",
        "    # loop through the universes of planes\n",
        "    for universe_id in range(num_universes_to_use):\n",
        "\n",
        "        # get the set of planes from the planes_l list, for this particular universe_id\n",
        "        planes = planes_l[universe_id]\n",
        "\n",
        "        # get the hash value of the vector for this set of planes\n",
        "        hash_value = hash_value_of_vector(v, planes)\n",
        "\n",
        "        # get the hash table for this particular universe_id\n",
        "        hash_table = hash_tables[universe_id]\n",
        "\n",
        "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
        "        document_vectors_l = hash_table[hash_value]\n",
        "\n",
        "        # get the id_table for this particular universe_id\n",
        "        id_table = id_tables[universe_id]\n",
        "\n",
        "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
        "        new_ids_to_consider = id_table[hash_value]\n",
        "\n",
        "        # remove the id of the document that we're searching\n",
        "        if doc_id in new_ids_to_consider:\n",
        "            new_ids_to_consider.remove(doc_id)\n",
        "            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
        "\n",
        "        # loop through the subset of document vectors to consider\n",
        "        for i, new_id in enumerate(new_ids_to_consider):\n",
        "\n",
        "            # if the document ID is not yet in the set ids_to_consider...\n",
        "            if new_id not in ids_to_consider_set:\n",
        "                # access document_vectors_l list at index i to get the embedding\n",
        "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
        "                document_vector_at_i = document_vectors_l[i]\n",
        "                vecs_to_consider_l.append(document_vector_at_i)\n",
        "\n",
        "                # append the new_id (the index for the document) to the list of ids to consider\n",
        "                ids_to_consider_l.append(new_id)\n",
        "\n",
        "                # also add the new_id to the set of ids to consider\n",
        "                # (use this to check if new_id is not already in the IDs to consider)\n",
        "                ids_to_consider_set.add(new_id)\n",
        "\n",
        "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
        "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
        "\n",
        "    # convert the vecs to consider set to a list, then to a numpy array\n",
        "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
        "\n",
        "    # call nearest neighbors on the reduced list of candidate vectors\n",
        "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
        "\n",
        "    # Use the nearest neighbor index list as indices into the ids to consider\n",
        "    # create a list of nearest neighbors by the document ids\n",
        "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
        "                            for idx in nearest_neighbor_idx_l]\n",
        "\n",
        "    return nearest_neighbor_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BO8FPSnienN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ff7c63-17ae-482e-f690-0a273c42cad4"
      },
      "source": [
        "# Fonction recherchant le voisin le plus proche \n",
        "nearest_neighbor_ids = approximate_knn(doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5) \n",
        "\n",
        "print(nearest_neighbor_ids)\n",
        "\n",
        "\n",
        "print(f\"Nearest neighbors for document {doc_id}\")\n",
        "print(f\"Document contents: {doc_to_search}\")\n",
        "print(\"\")\n",
        "\n",
        "for neighbor_id in nearest_neighbor_ids:\n",
        "  \n",
        "  print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
        "  print(f\"document contents: {d3_lem[neighbor_id]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fast considering 0 vecs\n",
            "[]\n",
            "Nearest neighbors for document 0\n",
            "Document contents: [\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dFgJOTEVTxd"
      },
      "source": [
        "### Seconde m√©thode : spaCy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtyC4gMkmnZ3"
      },
      "source": [
        "**Tokenisation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0O5bL6iVXAE"
      },
      "source": [
        "def return_token(sentence):\n",
        "    # Tokeniser la phrase\n",
        "    doc = nlp(sentence)\n",
        "    # Retourner le texte de chaque token\n",
        "    return [X.text for X in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "BKtTJe0umi_I",
        "outputId": "64d2ce1f-dfc8-4540-ff89-c2f680a82a42"
      },
      "source": [
        "return_token(cat3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-1ccc5ea3f888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreturn_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-07da97e8ea59>\u001b[0m in \u001b[0;36mreturn_token\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreturn_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Tokeniser la phrase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Retourner le texte de chaque token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGa9yDBNmqlc"
      },
      "source": [
        "**Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "6DpO0Xo8mmIy",
        "outputId": "db97ac8a-e96a-4e9b-a43c-9f4da55dae34"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('french'))\n",
        "\n",
        "clean_words = []\n",
        "for token in return_token(cat3):\n",
        "    if token not in stopWords:\n",
        "        clean_words.append(token)\n",
        "\n",
        "clean_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-7f9b9f5e1264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreturn_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopWords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mclean_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'return_token' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tw0aQ5447ym"
      },
      "source": [
        "## Trouver centroids "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYtSKuHIcPVb"
      },
      "source": [
        "Dans cette partie, un travail de recherche et d'essais est fait sur les fonctionnalit√©s de Kmeans. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaKmmDmOfhEf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "7064345b-78df-4482-abe1-a8a3c87b1679"
      },
      "source": [
        "'''\n",
        "# Function: K Means\n",
        "# -------------\n",
        "# K-Means is an algorithm that takes in a dataset and a constant\n",
        "# k and returns k centroids (which define clusters of data in the\n",
        "# dataset which are similar to one another).\n",
        "def kmeans(dataSet, k):\n",
        "\t\n",
        "    # Initialize centroids randomly\n",
        "    numFeatures = dataSet.getNumFeatures()\n",
        "    centroids = getRandomCentroids(numFeatures, k)\n",
        "    \n",
        "    # Initialize book keeping vars.\n",
        "    iterations = 0\n",
        "    oldCentroids = None\n",
        "    \n",
        "    # Run the main k-means algorithm\n",
        "    while not shouldStop(oldCentroids, centroids, iterations):\n",
        "        # Save old centroids for convergence test. Book keeping.\n",
        "        oldCentroids = centroids\n",
        "        iterations += 1\n",
        "        \n",
        "        # Assign labels to each datapoint based on centroids\n",
        "        labels = getLabels(dataSet, centroids)\n",
        "        \n",
        "        # Assign centroids based on datapoint labels\n",
        "        centroids = getCentroids(dataSet, labels, k)\n",
        "        \n",
        "    # We can get the labels too by calling getLabels(dataSet, centroids)\n",
        "    return centroids\n",
        "    \n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Function: K Means\\n# -------------\\n# K-Means is an algorithm that takes in a dataset and a constant\\n# k and returns k centroids (which define clusters of data in the\\n# dataset which are similar to one another).\\ndef kmeans(dataSet, k):\\n\\t\\n    # Initialize centroids randomly\\n    numFeatures = dataSet.getNumFeatures()\\n    centroids = getRandomCentroids(numFeatures, k)\\n    \\n    # Initialize book keeping vars.\\n    iterations = 0\\n    oldCentroids = None\\n    \\n    # Run the main k-means algorithm\\n    while not shouldStop(oldCentroids, centroids, iterations):\\n        # Save old centroids for convergence test. Book keeping.\\n        oldCentroids = centroids\\n        iterations += 1\\n        \\n        # Assign labels to each datapoint based on centroids\\n        labels = getLabels(dataSet, centroids)\\n        \\n        # Assign centroids based on datapoint labels\\n        centroids = getCentroids(dataSet, labels, k)\\n        \\n    # We can get the labels too by calling getLabels(dataSet, centroids)\\n    return centroids\\n    \\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu254EvXfim7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8b8d350e-fb3f-493c-e650-e6dae816aee5"
      },
      "source": [
        "\"\"\"# Function: Should Stop\n",
        "# -------------\n",
        "# Returns True or False if k-means is done. K-means terminates either\n",
        "# because it has run a maximum number of iterations OR the centroids\n",
        "# stop changing.\n",
        "def shouldStop(oldCentroids, centroids, iterations):\n",
        "    if iterations > MAX_ITERATIONS: return True\n",
        "    return oldCentroids == centroids\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Function: Should Stop\\n# -------------\\n# Returns True or False if k-means is done. K-means terminates either\\n# because it has run a maximum number of iterations OR the centroids\\n# stop changing.\\ndef shouldStop(oldCentroids, centroids, iterations):\\n    if iterations > MAX_ITERATIONS: return True\\n    return oldCentroids == centroids\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1SlCSO1flZw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "739491bc-d1a1-4e22-eb43-85793b4f579e"
      },
      "source": [
        "\"\"\"# Function: Get Labels\n",
        "# -------------\n",
        "# Returns a label for each piece of data in the dataset. \n",
        "def getLabels(dataSet, centroids):\n",
        "  \n",
        "    # For each element in the dataset, chose the closest centroid. \n",
        "    # Make that centroid the element's label.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Function: Get Labels\\n# -------------\\n# Returns a label for each piece of data in the dataset. \\ndef getLabels(dataSet, centroids):\\n  \\n    # For each element in the dataset, chose the closest centroid. \\n    # Make that centroid the element's label.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjKL7X5efo9L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8e4daf44-27db-4499-cfee-b3cf647a7fb2"
      },
      "source": [
        "\"\"\"# Function: Get Centroids\n",
        "# -------------\n",
        "# Returns k random centroids, each of dimension n.\n",
        "def getCentroids(dataSet, labels, k):\n",
        "    # Each centroid is the geometric mean of the points that\n",
        "    # have that centroid's label. Important: If a centroid is empty (no points have\n",
        "    # that centroid's label) you should randomly re-initialize it.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Function: Get Centroids\\n# -------------\\n# Returns k random centroids, each of dimension n.\\ndef getCentroids(dataSet, labels, k):\\n    # Each centroid is the geometric mean of the points that\\n    # have that centroid's label. Important: If a centroid is empty (no points have\\n    # that centroid's label) you should randomly re-initialize it.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7dNlvi19SO3"
      },
      "source": [
        "# R√©partition des donn√©es en train set et test set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6R725nstwdI"
      },
      "source": [
        "Le but final est de construire un algorithm capable de classifier chaque r√©ponse de toutes les cat√©gories de formulaires et d'en extraire un besoin. Nous voulons donc cr√©er un algorithm de machine learning ou deep-learning qui serait capable de comprendre les besoins formul√©s par les intervenants. \n",
        "\n",
        "Nous r√©fl√©chissons √† plusieurs mani√®res d'√©laborer un tel algorithm. \n",
        "\n",
        "Une option int√©ressante serait de relever manuellement quelques besoins r√©currents et d'entrainer notre algorithm √† les trouver dans notre texte. \n",
        "\n",
        "Une autre option serait de demander √† l'algorithm de trouver ces besoins par lui-m√™me gr√¢ce aux essais de clustering r√©alis√©s pr√©cedemment. Gr√¢ce aux vectorisations des mots, il pourrait ressembler les vecteurs les plus proches ou les plus similaires. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsO9iiffzlVK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "18ad766f-3355-446c-8937-2de846ec0862"
      },
      "source": [
        "'''#tokenized_data = tokenize_sentences(cat3_roots)\n",
        "#print(tokenized_data)\n",
        "random.seed(87)\n",
        "random.shuffle(d3_lem)\n",
        "\n",
        "train_size = int(len(d3_lem) * 0.8)\n",
        "train_data = d3_lem[0:train_size]\n",
        "test_data = d3_lem[train_size:]'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#tokenized_data = tokenize_sentences(cat3_roots)\\n#print(tokenized_data)\\nrandom.seed(87)\\nrandom.shuffle(d3_lem)\\n\\ntrain_size = int(len(d3_lem) * 0.8)\\ntrain_data = d3_lem[0:train_size]\\ntest_data = d3_lem[train_size:]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJqMbycK20Hy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "dc3a28cc-7ef3-498a-f569-f48d260b8f68"
      },
      "source": [
        "'''print(\"{} data are split into {} train and {} test set\".format(\n",
        "    len(d3_lem), len(train_data), len(test_data)))\n",
        "\n",
        "print(\"First training sample:\")\n",
        "print(train_data[0])\n",
        "print(train_data)\n",
        "      \n",
        "print(\"First test sample\")\n",
        "print(test_data[0])'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'print(\"{} data are split into {} train and {} test set\".format(\\n    len(d3_lem), len(train_data), len(test_data)))\\n\\nprint(\"First training sample:\")\\nprint(train_data[0])\\nprint(train_data)\\n      \\nprint(\"First test sample\")\\nprint(test_data[0])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqREuz_zpXk8"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geAwKUaXWSNz"
      },
      "source": []
    }
  ]
}